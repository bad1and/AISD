# AISD

## Список вопросов
1. [[#1. Понятие алгоритма, его свойства и формы записи.(по Кормену)]]
2.  [[#2. Понятие алгоритмической сложности и сложности в зависимости от входных параметров (в худшем, среднем и лучшем случаях)]]
3. [[#3. Понятие асимптотической оценки сложности (Ο, Θ, Ω)]] 
4. [[#4. Понятие рекуррентных отношений и методы определения асимптотической оценки]]
5. [[#5. Описание и реализация алгоритмов линейного и бинарного поиска.]]
6. [[#6. Понятие алфавита в теории формальных языков и реализация простого поиск цепочки символов]]
7. [[#7. Описание и реализация алгоритма Рабина-Карпа с использованием хэш-функции]]
8. [[#8. Описание и реализация алгоритма Кнута-Морриса-Пратта с использованием префикс-функции]]
9.  [[#9. Описание и реализация алгоритма Бойера-Мура с использованием эвристики стоп-символа]]
10. [[#10. Описание структуры и реализация методов списка с использованием динамического массива]]
11. [[#11. Описание структуры и реализация методов двусвязного списка]]
12. [[#12. Описание структуры и методов очереди, кольцевой очереди, стека, деки]]
13. [[#13. Реализация алгоритма сортировочной станции]]
14. [[#14. Описание структуры и методов ассоциативного массива через реализацию в виде хэш-таблицы]]
15. [[#15. Описание структуры двоичного дерева и реализации методов обхода дерева]]
16. [[#16. Описание структуры двоичного дерева поиска и реализации методов добавления и удаления узла]]
17. [[#17. Описание структуры сбалансированных деревьев поиска и реализация методов малых и больших поворотов узла]]
18. [[#18. Описание структуры АВЛ-дерева и метода добавление нового 1. узла]]
19. [[#19. Описание структуры АВЛ-дерева и метода удаления существующего узла]]
20. [[#20. Описание структуры Красно-Черного дерева и метода добавление нового узла]]
21. [[#21. Описание структуры Красно-Черного дерева и метода удаления существующего узла]]
22. [[#22. Описание структуры B-дерева и реализация алгоритма поиска ключа]]
23. [[#23. Описание метода добавления нового ключа в B-дерева]]
24. [[#24. Описание метода удаления существующего ключа из B-дерева]]
25. [[#25. Описание и реализация алгоритма пузырьковой сортировки (bubble)]]
26. [[#26. Описание и реализация алгоритма сортировки вставками (insertion)]]
27. [[#27. Описание и реализация алгоритма селекционной сортировки (selection)]]
28. [[#28. Описание и реализация алгоритма сортировки слияниями (merge-sort)]]
29. [[#29. Описание и реализация алгоритма быстрой сортировки (quick-sort)]]
30. [[#30. Описание и реализация алгоритма пирамидальной сортировки (heap-sort)]]
31. [[#31. Описание и реализация алгоритма поразрядных, блочных сортировок и сортировки подсчётом]]
32. [[#32. Описания способов хранения графов, а также реализация поиска в ширину и алгоритма поиска кратчайшего пути в незвзвешанном графе]]
33. [[#33. Описания способов хранения графов, а также реализация поиска в глубину и алгоритма топологической сортировки]]
34. [[#34. Описание и реализация алгоритма Прима для поиска минимального остовного дерева]]
35. [[#35. Описание и реализация алгоритма Крускала для поиска минимального остовного дерева]]


## 1. Понятие алгоритма, его свойства и формы записи.(по Кормену)

>**Алгоритм** (algorithm) — это любая корректно определенная вычислительная процедура, на вход (input) которой подается некоторая величина или набор величин, и результатом выполнения которой является выходная (output) величина или набор значений. Таким образом, алгоритм представляет собой последовательность вычислительных шагов, преобразующих входные величины в выходные.

Алгоритм также можно рассматривать как инструмент, предназначенный для решения корректно поставленной вычислительной задачи (computational problem).

##### Свойства алгоритмов:
- **Корректность:** Алгоритм считается корректным, если для каждого допустимого ввода он выдаёт корректный результат. Некорректные алгоритмы либо завершаются с ошибкой, либо дают неверный ответ.
- **Определённость:** Алгоритм должен чётко описывать действия, исключая любую двусмысленность.
- **Конечность:** Алгоритм должен завершаться за конечное время.
- **Массовость:** Один алгоритм может быть применим для множества различных экземпляров одной и той же задачи.
##### Доп свойства из разных источников:
- **Эффективность:** Алгоритм должен использовать минимально возможное количество ресурсов (времени, памяти).
- **Дискретность (пошаговость):** Алгоритм состоит из отдельных шагов (инструкций), которые выполняются последовательно.

##### Формы записи алгоритмов: 
Алгоритмы можно задавать различными способами:
- **На естественном языке:** Простое описание действий.
- **В виде программного кода:** Алгоритм реализуется как программа на языке программирования.
- **На аппаратном уровне:** В некоторых случаях алгоритмы реализуются непосредственно в виде аппаратного обеспечения.
- **Блок-схема**  
    Графическое представление алгоритма, где шаги изображаются в виде фигур (блоков), соединенных линиями для указания последовательности.

В общем случае важным требованием является предоставление точного и однозначного описания последовательности действий.

##### Примеры применения алгоритмов:
- Сортировка данных.
- Построение кратчайшего пути между точками.
- Решение задач оптимизации, например, "задачи о коммивояжёре".
- Биологические исследования, например, расшифровка генома.
- Работа с большими объёмами данных, в том числе в Internet и электронной коммерции.

Алгоритмы находят применение во всех областях науки и техники, являясь основой автоматизации и решения сложных задач.

## 2. Понятие алгоритмической сложности и сложности в зависимости от входных параметров (в худшем, среднем и лучшем случаях)

[Материалы на сайте Пелевина](https://markoutte.me/students/algo-complexity/)

>Временные затраты — "Временная сложность"
>Затраты по памяти — "Пространственная сложность"

Самый простой способ оценить сложность алгоритма — это сосчитать сколько инструкций (действий) выполняются за время работы алгоритма

#### Временная сложность n
Такая сложность будет возникать при работе циклов. Суммарное количество инструкций в таком случае получается n×(кол-во инстркуций), где n — число итераций цикла. Если же n явно не задано, то говорят, что алгоритм линейно зависит от размера входных данных

```python
def findMax(lst):
    max_val = lst[0]
    for i in range(len(lst)):
        if lst[i] > max_val:
            max_val = lst[i]
    return max_val
```

#### Временная сложность n² или n×m
В случае, если у нас появляются какие-либо вложенные циклы, то итоговое количество итераций перемножается. То есть суммарное количество инструкций = n×m×(кол-во инстркуций). 
В частном случае, если n=m, то можно говорить о "квадратичной зависимости"
```python
def isIdentityMatrix(mat):
    for i in range(len(mat)):
        row = mat[i]
        if len(row) != len(mat):
            return False
        for j in range(len(mat)):
            if j == i and row[j] != 1:
                return False
            if j != i and row[j] != 0:
                return False
    return True
```

#### Пример для понимания разницы в сложности 
Пусть есть два алгоритма, решающих одну и ту же задачу, но один за $2n^2$, а другой за $50n\log_{2}{n}$. Так же есть два компьютера: А и Б, где А решает $10^{10}\frac{\text{команд}}{\text{секунду}}$, а Б решает $10^{7}\frac{\text{команд}}{\text{секунду}}$. То есть первый в 1000 раз быстрее второго.

Компьютер А, скорость = $10^{10}\frac{\text{команд}}{\text{секунду}}$
$$\frac{2×{10^7}^2 \text{ команд}}{10^{10} \text{ команд в секунду}}= 20 000 \text{ секунд} = 5,5(5) \text{ часов}$$

Компьютер Б, скорость = $10^{7}\frac{\text{команд}}{\text{секунду}}$
$$\frac{50×10:{7}×\log_{2}{10^{7}} \text{ команд}}{10^{10} \text{ команд в секунду}}= 1163 \text{ секунд} = 19,4 \text{ минуты}$$

Как мы можем видеть, число инструкций влияет гораздо сильнее на время работы алгоритма нежели увеличение производительности.

#### Различные названия роста целевой функции $T(n)$:
1. Константное (время работы не зависит от $n$)
2. Логарифмическое($\log{n}$: напр., двоичный поиск)
3. Линейное ($n$: напр., поиск максимального значения)
4. Квазилинейное ($n\log{n}$: напр., большинство эффективных сортировок)
5. Квадратичное ($n^2$: напр., обход значений матрицы)
6. Полиномиальное ($n^c \text{ для c > 1}$ )
7. Экспоненциальное ($c^n \text{ для c > 1}$ )
8. Факториальное ($n!$: напр., задача коммивояжёра)

#### Лучший и худший случай работы алгоритма
Как пример — сортировка пузырьком. Как видно из кода, её сложность в среднем $n^2$
```python
def bubbleSort(arr):
    n = len(arr)
    for i in range(n - 1):
        swapped = False
        for j in range(n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                swapped = True
        if not swapped:
            break
```

Однако, если же массив уже будет отсортирован, то алгоритм один единственный раз пройдётся по массиву, то есть $n$ раз выполнить проверку на отсортироанность, и закончит работу. А если же массив будет неотсортированн, то сложность будет $n^2$. 
Выходит, что в лучшем случае сортировка пузырьком работает за $n$, а в худшем за $n^2$.

#### Как посчитать сложность алгоритма в среднем
Для выполнения подсчёта в среднем используется вероятностный анализ алгоритма. 
Суть такая — расчитывается вероятность получить какие-то данные на вход, а потом расчитывается математическое ожидание.

> [!Как математически посчитать]
 >Обозначим время работы некоторого алгоритма $A$, на вход которого подаётся вход $x$, как $C_A(x)$. Для определения среднего случая рассмотрим конечное множество $X_n=\{x : ||x|| = n\}$ входов размеров n. Предположим, что каждому входу $x \in X_n$ приписана вероятность $P_n(x)$. На заданном таким образом вероятностном пространстве сложностью в среднем называют математическое ожидание соответствующей случайной величины:
 >$$T_A(n) = \sum_{x \in X_n} C_A(x)P_n(x)$$


## 3. Понятие асимптотической оценки сложности (Ο, Θ, Ω)
Используется вместо точной оценки временной сложности
#### Θ(g(n)) — асимптотически точная граница
Будет находится между верхней и нижней границами функций алгоритма.
Показывает, что функция растет с точно такой скоростью
Используется когда важна точная оценка
- Определение:
  ![[Theta.png]]
- График:
  ![[Theta-graph.png]]

#### Ο(g(n)) — асимптотически верхняя граница
Все функции будут находиться ниже такой границы.
Показывает максимальную скорость роста функции
Часто используется для оценки наихудшего сценария
- Определение:
  ![[Big-O.png]]
- График:
  ![[Big-O-graph.png]]
  
#### Ω(g(n)) — асимптотически нижняя граница
Все функции будут находится выше такой границы
Показывает минимальную скорость роста функцииы
Часто используется для оценки лучшего сценария
- Определение:
  ![[Omega.png]]
- График:
  ![[Omega-graph.png]]


>Если обозначено малыми буквами, это означает строгую оценку. Значит что скорость не совпадает с $f(n)$ асимптотически, с заданной стороны









## 4. Понятие рекуррентных отношений и методы определения асимптотической оценки

Рекуррентное соотношение (рекурсия по человечески) – это уравнение задающее последовательность n через её значения при меньших n

$f(0)=const$
$f(n)=F(n, f(0), f(1), ... f(n-1))$

Примером рекурсии может быть факториал числа:

$0! = 1$
$n!=1*2 *...*(n-1)*n$

Асимптотическая оценка – обобщённая оценка сложности (по времени или памяти) выполнения алгоритма  
  - $О(1)$ означает, что данной операции требуется константное время. Например, за константное время выполняется поиск элемента в хэш-таблице, так как вы напрямую запрашиваете какой-то элемент, не делая никаких сравнений. То же самое относится к вызову i-того элемента в массиве.
  - $О(n)$ зависит от количества входных данных. В случае с линейными алгоритмами в худшем случае вам придется провести какую-то операцию с каждый элементом.
  - $O(\log n )$ - Каждая операция уменьшает количество входных данных вдвое.
  - $O(n\log n)$ можно представить в виде комбинации $O(\log n )$ и $O(n)$. 
    ```python
    for (int i = 0; i < n; i++) //выполняется n раз
	    for (int j = 1; j < n; j = j * 2) // выполняется log раз за 1                                               итерацию верхнего цикла
	```
  - $O(n^2)$ легко можно получить, если в программе будут присутствовать вложенные for-циклы, работающих в $O(n)$.

## 5. Описание и реализация алгоритмов линейного и бинарного поиска.

#### Линейный поиск 
Aлгоритм поиска элемента в массиве или списке, который последовательно проверяет каждый элемент до тех пор, пока не найдёт совпадение или не проверит все элементы.

Временная сложность: Худшая: $O(n)$ | Средняя: $O(n)$ | Лучшая: $O(1)$

```python
def liner_search(alf , text):
    for x in range(len(alf)):
        if alf[x] == text:
            return x
    return None
```

#### Бинарный поиск 
Aлгоритм поиска элемента в массиве, который последовательно делит пополам заранее отсортированный массив и сравнивает серединный элемент с искомым и продолжает делить массив пока не найдёт искомый элемент.

Временная сложность: Худшая: O(log n), Средняя: O(log n), Лучшая: O(1).
```python
def binary_search(alf, left, right, text):
    while True:
        middle = (left + right) // 2
        if text < alf[middle]:
            right = middle - 1
        if text > alf[middle]:
            left = middle + 1
        else:
            return middle
        if left > right:
            return None
```



## 6. Понятие алфавита в теории формальных языков и реализация простого поиск цепочки символов

#### Понятие алфавита в теории формальных языков
В теории формальных языков **алфавит** — это конечное множество символов, из которых строятся строки. Символы могут быть буквами, цифрами или другими знаками. Примеры алфавитов:

- Двоичный алфавит: {0, 1}.
- Латинский алфавит: {A, B, C, ... , Z}.
- Алфавит цифр: {0, 1, 2, ... , 9}.

> **Строка** — это конечная последовательность символов, составленная из алфавита. Например, для алфавита {A, B} строками являются: "ABBA", "A", "" (пустая строка).

Алфавиты лежат в основе определения формальных языков, которые описывают множество строк, удовлетворяющих определённым правилам.

Обозначим через Σ∗ множество всех строк конечной длины, образованных с помощью символов алфавита Σ. В этой главе рассматриваются только строки конечной длины. Пустая строка (empty string) конечной длины, которая обозначается ε, также принадлежит множеству Σ∗. Длина строки x обозначается |x|. Конкатенация (concatenation) двух строк x и y, которая обозначается xy, имеет длину |x| + |y| и состоит из символов строки x, после которых следуют символы строки y.

#### Реализация простого поиска цепочки символов

**Простой (наивный) поиск**
Алгоритм заключается в последовательном переборе с сравнением символов строки и образца:

```python
def naive_search(text, pattern):
    n = len(text)
    m = len(pattern)

    for i in range(n - m + 1):
        for j in range(m):
            if text[i + j] != pattern[j]:
                break
        else:  # выполняется, если цикл завершился без break
            return i

    return -1
```

**i** отвечает за сдвиг образца, а внутренний цикл проверяет совпадение символов строки с индексом i+j и символа образца с индексом j. Если символы совпадают на всём участке, подстрока найдена (индекс **i**). 
Сложность: **O(N * M)**, где N — длина строки S, а M — образца P.

#### Алгоритм Рабина — Карпа
Улучшает наивный поиск за счёт хэш-функции, преобразующей строку в числовое значение.

Хэш-функция преобразует строку в числовое значение. Пример:
- Метод деления: $hash(k) = k\mod r$
- Метод середины квадрата: берётся средняя часть квадрата числа.
- Метод умножения: $hash(k)=[r[\frac{A}{w}*k]]$, 
   $w$ - размер машинного слова,  $A$ - взаимно простое число к $w$,   $r$ - количество доступных значений хэш-функции

**Коллизии** возникают, когда разные элементы отображаются в одно значение. Идеальное хэширование исключает коллизии. Для разрешения коллизий в хэш-таблицах используются метод цепочек или открытой адресации.

Пример тривиальной хэш-функции: сумма индексов символов строки. При сдвиге обновляется хэш-функция, что ускоряет сравнение. При совпадении проводится проверка строк посимвольно (из-за возможных коллизий).

```python
def rabin_karp_search(S, P):
    def simple_hash(s):
        h = 0
        for c in s:
            h += ord(c)
        return h

    n = len(S)
    m = len(P)

    if m == 0 or n < m:
        return -1

    original_hash = simple_hash(P)
    temp_hash = simple_hash(S[0:m])

    for i in range(n - m + 1):
        if original_hash == temp_hash:
            if S[i:i + m] == P:
                return i

        if i < n - m:
            temp_hash = temp_hash - ord(S[i]) + ord(S[i + m])

    return -1
```

Сложность зависит от скорости вычисления хэша.

#### Алгоритм Бойера — Мура
Сравнение начинается с конца строки, что позволяет пропускать сразу несколько символов. При несовпадении символа сдвиг определяется таблицей, содержащей индексы символов образца:

```python
def boyer_moore_search(S, P):
    table = {}
    for i in range(len(P)):
        table[P[i]] = i
    
    i = 0
    while i <= len(S) - len(P):
        j = len(P) - 1
        while j >= 0:
            if P[j] != S[i + j]:
                step = j - table.get(S[i + j], -1)
                i += max(step, 1)
                break
            j -= 1
        else:
            return i
    return -1
```
Эффективность: **O(N + M)**.

#### Алгоритм Кнута — Морриса — Пратта (КМП)
Наиболее эффективный алгоритм. Основой является вычисление префикс-функции, показывающей длину наибольшего префикса, совпадающего с суффиксом.

Алгоритм подсчёта:
```python
def kmp_search(S, P):
    C = P + '#' + S
    PI = [0] * len(C)
    
    for i in range(1, len(C)):
        j = PI[i - 1]
        while j > 0 and C[j] != C[i]:
            j = PI[j - 1]
        if C[j] == C[i]:
            PI[i] = j + 1
        if PI[i] == len(P):
            return True
    return False
```
Префикс-функция строится за линейное время, после чего осуществляется поиск образца.

## 7. Описание и реализация алгоритма Рабина-Карпа с использованием хэш-функции

[Материалы на сайте Пелевина](https://markoutte.me/students/substring/)

Для реализации этого алгоритма нужно реализовать простую хэш-функцию для строки. Функция будет работать так — берётся номер каждого символа строки (весь алфовит пронумерован) и потом складываются все индексы. Пример $hash(AABBCC) = 1+1+2+2+3+3 = 12$. Такая хэш-функция называется "скользящий хэш".

> [!Как работает данный алгоритм]
> 1. Считаем хэш для искомой подстроки
> 2. Начинаем двигаться по изначальной строке посимвольно. Соответсвенно при сдвиге на один символ новый хэш будет считаться как старый хэш минус индекс прерыдушего символа плюс хэш следующего символа. 
> 3. При каждом сдвиге на $+1$ символ мы сверяем новый хэш со старым. Если совпадает, то мы нашли индекс, с которого начинается искомая строка

```python
def rabin_karp_search(S, P):
    def hash(s):
        h = 0
        for char in s:
            h += ord(char)
        return h
    
    n = len(S)
    m = len(P)
    
    if m == 0 or n < m:
        return -1
    
    original_hash = hash(P)
    temp_hash = hash(S[0:m])
    
    for i in range(n - m + 1):
        if original_hash == temp_hash and S[i:i + m] == P:
            return i
        if i < n - m:
            temp_hash = temp_hash - ord(S[i]) + ord(S[i + m])
    
    return -1
```




## 8. Описание и реализация алгоритма Кнута-Морриса-Пратта с использованием префикс-функции
Описание алгоритма:
Для поиска подстроки достаточно склеить образец и строку с помощью не входящего в алфавит символа, после чего начать считать префикс-функцию. (Это не обязательно, можно отдельно вычислить префикс и использовать его для поиска)
Как только значение функции = длине образца — подстрока найдена.
- Предназначен для поиска подстроки в строке
- Работает за линейное время (в случае наивного подсчета префикс-функции за квадратичное)

> **Префикс-функция**  $\pi(S, i)$ — вычисляет длину наибольшего собственного (не равного самой подстроке) префикса совпадающего с суффиксом этой подстроки
	       *Пример: строка `котокот` префикс `кот` длины 3 совпадает с суффиксом*


Основная идея эффективного алгоритма подсчета префикс-функции в том,  что при добавления нового символа к подстроке можно использовать уже вычисленные значения префикс-функции.
Почему это работает: 
- При добавлении нового символа подстроки, может получится что увеличенный суффикс совпадает с префиксом
  *новый символ* $i+1$, $\pi(S, i)=k$,  *получаем* $S[i + 1] = S[k + 1]$
- Если они не совпадают, пробуем посмотреть на меньший суффикс. Так как префикс-функция уже посчитана, полагаем что $k = \pi (S, k)$ и снова проверяем $S[i + 1] = S[k + 1]$.
- Если в какой-то момент $k=0$, полагаем что $\pi(S, i+1)=0$

Реализация:
Просто проверяем наличие, в целом можно возвращать индексы совпадения, тогда посчитаем вхождения
```python
//вычисление префикс-функции
def computePrefixFunction(s):
    n = len(s)
    pi = [0] * n
    
    for i in range(1, n):
        j = pi[i - 1]
        while j > 0 and s[i] != s[j]:
            j = pi[j - 1]
        if s[i] == s[j]:
            j += 1
        pi[i] = j
    
    return pi

def KMPSearch(pattern, text):
    combined = pattern + "#" + text
    pi = computePrefixFunction(combined)
    
    m = len(pattern)
    for i in range(m + 1, len(combined)):
        if pi[i] == m:
            return True
    
    return False
```






## 9. Описание и реализация алгоритма Бойера-Мура с использованием эвристики стоп-символа
Алгоритм Бойера-Мура – алгоритм поиска подстроки в строке. Он работает по принципу смещения в зависимости от совпадения символов в строке с искомой подстрокой.

**Алгоритм Бойера-Мура делится на 2 этапа:**
1.    Заполнение таблицы смещений(сдвигов)
2.    Поиск подстроки в строке со сдвигом подстроки.

#### 1.  Формируем таблицу смещений
Пускай есть искомая подстрока – «данные».  
По подстроке заполняем таблицу, в которой будут написаны сдвиги при нахождении этих символов в строке (звёздочка – любой другой символ, который не учитывается в основной таблице):

| Индекс |     |     |     |     |     |     |     |
| ------ | --- | --- | --- | --- | --- | --- | --- |
| Символ | Д   | А   | Н   | Н   | Ы   | Е   | *   |
| Сдвиг  |     |     |     |     |     |     |     |

Пишем индексы с конца, начиная с нуля. (Е – 0, Ы – 1, Н – 2,…)

| Индекс | 5   | 4   | 3   | 2   | 1   | 0   |     |
| ------ | --- | --- | --- | --- | --- | --- | --- |
| Символ | Д   | А   | Н   | Н   | Ы   | Е   | *   |
| Сдвиг  |     |     |     |     |     |     |     |

Далее, начиная с **первого** (не нулевого, а именно первого), спускаем индексы в строку смещений:

| Индекс | 5   | 4   | 3   | 2   | 1   | 0   |     |
| ------ | --- | --- | --- | --- | --- | --- | --- |
| Символ | Д   | А   | Н   | Н   | Ы   | Е   | *   |
| Сдвиг  | 5   | 4   | 3   | 2   | 1   |     |     |

В случае, если в подстроке имеется 2 одинаковых символа, то обоим им их меньшее значение.

| Индекс | 5   | 4   | 3   | 2   | 1   | 0   |     |
| ------ | --- | --- | --- | --- | --- | --- | --- |
| Символ | Д   | А   | Н   | Н   | Ы   | Е   | *   |
| Сдвиг  | 5   | 4   | 2   | 2   | 1   |     |     |

Затем, если 0-вой символ (_в нашем случае «Е»_) нигде не попадался, то присваиваем ему и звёздочке сдвиг, равный длине подстроки (_в слове «данные» 6 букв, так что звёздочка = 6_)_._

| Индекс | 5   | 4   | 3   | 2   | 1   | 0   |     |
| ------ | --- | --- | --- | --- | --- | --- | --- |
| Символ | Д   | А   | Н   | Н   | Ы   | Е   | *   |
| Сдвиг  | 5   | 4   | 2   | 2   | 1   | 6   | 6   |

**Исключение:**  
Если же 0-вой символ уже встречался в строке, то ему присоим значение, уже ранее заполненное в таблице, а звёздочка так и будет содержать длину подстроки.

| Индекс | 5   | 4   | 3   | 2   | 1   | 0   |     |
| ------ | --- | --- | --- | --- | --- | --- | --- |
| Символ | Д   | Е   | Н   | Н   | Ы   | Е   | *   |
| Сдвиг  | 5   | 4   | 2   | 2   | 1   | 4   | 6   |

---

#### 2.  Поиск подстроки в строке

Пусть строка будет:  «Большие метеоданные»
Искомая подстрока:  «данные»


Таблица смещений:  

| Д   | А   | Н   | Н   | Ы   | Е   | *   |
| --- | --- | --- | --- | --- | --- | --- |
| 5   | 4   | 2   | 2   | 1   | 6   | 6   |

Начинаем сопоставление, опираясь на таблицу смещений:  

| б   | о   | л   | ь   | ш   | и   | е   |     | м   | е   | т   | е   | о   | д   | а   | н   | н   | ы   | е   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| д   | а   | н   | н   | ы   | е   |     |     |     |     |     |     |     |     |     |     |     |     |     |

Сравнивая **последний символ** в подстроке и строке замечаем несовпадение («е» != «и»).
Тогда смотрим на то, какой символ не совпал в самой **строке** («и» в данном случае) и ищем несовпавший символ в таблице смещений:   

| Д   | А   | Н   | Н   | Ы   | Е   | *   |
| --- | --- | --- | --- | --- | --- | --- |
| 5   | 4   | 2   | 2   | 1   | 6   | 6   |

Т.к. символа «и» нету среди основных символов  
«д», «а», «н», «н», «ы», «е», то будем относиться к нему как к любому символу (смотрим на *)

Под звёздочкой написано 6, так что смещаем нашу подстроку на 6 символов вправо.

| б   | о   | л   | ь   | ш   | и   | е   |     | м   | е   | т   | е   | о   | д   | а   | н   | н   | ы   | е   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|     |     |     |     |     |     | д   | а   | н   | н   | ы   | е   |     |     |     |     |     |     |     |

Замечаем, что последние символы совпадают, так что проверяем предпоследние («т» != «ы»).

В случае, если несовпадение производится **не** с последним символом, то сдвиг производится по несовпавшему символу **подстроки** («ы» в данном случае)

| Д   | А   | Н   | Н   | Ы   | Е   | *   |
| --- | --- | --- | --- | --- | --- | --- |
| 5   | 4   | 2   | 2   | 1   | 6   | 6   |

Для «ы» смещение = 1, поэтому сдвигаем подстроку на 1 вправо.

| б   | о   | л   | ь   | ш   | и   | е   |     | м   | е   | т   | е   | о   | д   | а   | н   | н   | ы   | е   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|     |     |     |     |     |     |     | д   | а   | н   | н   | ы   | е   |     |     |     |     |     |     |

Замечаем, что «о» и «е» не совпадают и «о» нету в таблице сдвигов, поэтому сдвигаем строку на 6 влево.

| б   | о   | л   | ь   | ш   | и   | е   |     | м   | е   | т   | е   | о   | д   | а   | н   | н   | ы   | е   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|     |     |     |     |     |     |     |     |     |     |     |     |     | д   | а   | н   | н   | ы   | е   |

Если бы была такая ситуация:

| …   | …   | м   | е   | т   | е   | о   | д   | а   | н   | н   | ы   | е   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| …   | …   | д   | а   | н   | н   | ы   | е   |     |     |     |     |     |

| Д   | А   | Н   | Н   | Ы   | Е   | *   |
| --- | --- | --- | --- | --- | --- | --- |
| 5   | 4   | 2   | 2   | 1   | 6   | 6   |

То обращаясь к таблице сдвигов, заметим, что под «д» стоит 5, так что сдвинем подстроку на 5 вправо и тоже получим готовую строку.

| …   | …   | м   | е   | т   | е   | о   | д   | а   | н   | н   | ы   | е   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| …   | …   |     |     |     |     |     | д   | а   | н   | н   | ы   | е   |
## 10. Описание структуры и реализация методов списка с использованием динамического массива

#### Динамический массив 
Массив, размер которого может изменяться во время выполнения программы.

```python
class DynamicArray{  
private:  
            int* data;  
            int capacity, size;  
            void resize(){  
                        capacity *= 2;  
                        int* newData = new int[capacity];  
                        for (int i = 0; i < size; i++) {  
                                   newData[i] = data[i];  
                        }  
                        delete[] data;  
                        data = newData;  
            }  
public:  
            DynamicArray(): size(0), capacity(10) {  
                        data = new int[capacity];  
            }  
            ~DynamicArray() { delete[] data };

            void add(int value) {  
                        if (size == capacity) resize();  
                        data[size] = value;  
                        size++;  
            }  
            void remove(int index) {  
                        if (index < 0 || index >= size) return;  
                        for (int i = index; i < size – 1; i++) {  
                                   data[i] = data[i+1];  
                        }  
                        size--;  
            }  
            void print(){  
                        for (int i = 0; i < size; i++){  
                                   cout << data[i] < “ “;  
                        }  
            }
```
```python
PYTHON
class DynamicArray:
    def __init__(self):
        self.capacity = 10
        self.size = 0
        self.data = [0] * self.capacity
    
    def __del__(self):
        pass
    
    def resize(self):
        self.capacity *= 2
        newData = [0] * self.capacity
        for i in range(self.size):
            newData[i] = self.data[i]
        self.data = newData
    
    def add(self, value):
        if self.size == self.capacity:
            self.resize()
        self.data[self.size] = value
        self.size += 1
    
    def remove(self, index):
        if index < 0 or index >= self.size:
            return
        for i in range(index, self.size - 1):
            self.data[i] = self.data[i + 1]
        self.size -= 1
    
    def print(self):
        for i in range(self.size):
            print(self.data[i], end=" ")
        print()
```

## 11. Описание структуры и реализация методов двусвязного списка

#### Двусвязный список 
Линейная структура данных, в которой каждый элемент содержит ссылки как на предыдущий, так и на следующий элементы, что позволяет легко перемещаться в обоих направлениях.
```python
struct mData{  
            int value;  
            mData* next = nullptr;  
            mData* pred = nullptr;  
}  
class DoubleLinkedList{  
private:  
            mData* head;

public:  
            DoubleLinkedList(): head (nullptr){}  
            ~DoubleLinkedList() {  
                        while (head != nullptr) {  
                                   mData* temp = head;  
                                   head = head -> next;  
                                   delete temp;  
                        }  
            }  
            void add (int value) {  
                        if (head == nullptr) {  
                                   head = new mData {value, nullptr, nullptr};  
                        } else {  
                                   mData *temp = head;  
                                   while (temp -> next != nullptr) {  
                                               temp = temp -> next;  
                                   }  
                                   mData* newNode = new mData{value, nullptr, temp};  
                                   temp -> next = newNode;  
            }}

void remove(int index) {  
            if (index < 0 || head == nullptr) return;  
            if (index == 0) {  
                        mData* temp = head;  
                        head = head -> next;  
                        if(head) head -> pred = nullptr;  
                        delete temp;  
                        return;  
            }  
            mData* current = head;  
            for (int i = 0; current != nullptr && i < index; i++) {  
                        current = current -> next;  
            }  
            if (current != nullptr) {  
                        if (current -> next) current -> next ->pred = current -> pred;  
                        if (current -> pred) current -> pred ->next = current -> next;  
                        delete current;  
            }
```

```python
PYTHON
class mData:
    def __init__(self, value):
        self.value = value
        self.next = None
        self.pred = None

class DoubleLinkedList:
    def __init__(self):
        self.head = None
    
    def __del__(self):
        while self.head is not None:
            temp = self.head
            self.head = self.head.next
            del temp
    
    def add(self, value):
        if self.head is None:
            self.head = mData(value)
        else:
            temp = self.head
            while temp.next is not None:
                temp = temp.next
            newNode = mData(value)
            newNode.pred = temp
            temp.next = newNode
    
    def remove(self, index):
        if index < 0 or self.head is None:
            return
        if index == 0:
            temp = self.head
            self.head = self.head.next
            if self.head:
                self.head.pred = None
            del temp
            return
        
        current = self.head
        i = 0
        while current is not None and i < index:
            current = current.next
            i += 1
        
        if current is not None:
            if current.next:
                current.next.pred = current.pred
            if current.pred:
                current.pred.next = current.next
            del current
```

## 12. Описание структуры и методов очереди, кольцевой очереди, стека, деки

#### Очередь 
– это линейная структура данных, которая следует принципу Первый пришёл – Первым обслужен (FIFO – First in first out). Элементы добавляются в конец (вставка) и удаляются с начала (извлечение).  
> Методы: push(item), pop(), back(), empty(), size().

#### Кольцевая очередь
– это расширенная версия обычной очереди, в которой последний элемент очереди соединён с первым элементом очереди, образующим цикл.  
> Методы: enqueue(item), dequeue(), front(), isFull(), isEmpty(), size().

#### Стек 
– это линейная структура данных, которая следует принципу Последним пришёл – Первым обслужен (LIFO – Last in first out).  
> Методы: push(item), pop(), top(), empty(), size().

#### Дек (двусторонняя очередь) 
– это обобщение очереди, где элементы могут добавляться и удаляться с обоих концов (FIFO).  
> Методы: push_front(item), push_back(item), pop_front(), pop_back(), front(), back(), empty(), size().

## 13. Реализация алгоритма сортировочной станции  
```python
PYTHON
def Prioritet(op):
    if op in ["+", "-"]:
        return 1
    if op in ["*", "/"]:
        return 2
    if op == "^":
        return 3
    if op in ["sin", "cos"]:
        return 4
    return 0

def is_left_associative(op):
    return op != "^"

def is_operator(token):
    return token in ["+", "-", "*", "/", "^", "sin", "cos"]

def is_number(token):
    return token and token[0].isdigit()

def toPostfix(expression):
    tokens = expression.split()
    output = []
    operators = []
    
    for token in tokens:
        if is_number(token):
            output.append(token)
        elif is_operator(token):
            while (operators and is_operator(operators[-1]) and 
                   ((is_left_associative(token) and Prioritet(token) <= Prioritet(operators[-1])) or 
                    (not is_left_associative(token) and Prioritet(token) < Prioritet(operators[-1])))):
                output.append(operators.pop())
            operators.append(token)
        elif token == "(":
            operators.append(token)
        elif token == ")":
            while operators and operators[-1] != "(":
                output.append(operators.pop())
            operators.pop()
    
    while operators:
        output.append(operators.pop())
    
    return " ".join(output)
```


## 14. Описание структуры и методов ассоциативного массива через реализацию в виде хэш-таблицы

**Ассоциативный массив** - абстрактная структура данных позволяющая хранить пары ключ-значение, где каждый ключ уникален.

**Хэш-таблица** — структура данных, использующая функцию хэширования для преобразования ключа в индекс массива, где хранится соответствующее значение. Операции в среднем выполняются за константу 
Компоненты хэш-таблицы:
1. Массив — основной контейнер со значениями (buckets, ячейки)
2. Функция хэширования: принимает ключ и возвращает индекс массива, содержащий значение
3. Разрешение коллизий: Обработка ситуаций когда два разных ключа имеют одинаковый хэш
#### Хэш-функция
Хэширование само по себе может выполняться для произвольного массива данных, выходным значением является битовый массив заданной длины.
##### Способы написания функции:
*множество $K$ чисел, считаем хэш-функцию в множество $R$*
1. **Метод деления** — входное число делится на количество доступных значений хэш-функции, результат это остаток от деления: $hash(k) = k\mod r$
2. **Метод середины квадрата** — входное число возводится в квадрат, а из его середины берется необходимое количество знаков
3. **Метод умножения** — $hash(k)=[r[\frac{A}{w}*k]]$, 
   $w$ - количество символов представимых машинным словом 
   $A$ - взаимно простое число к $w$ 
   $r$ - количество доступных значений хэш-функции
При любой функции коллизии не исключаются. Коллизии - ситуация когда у разных входных элементов одинаковое хэш-значение.
Если коллизии не случаются, то такое хэширование называют **идеальным хэшированием**.

##### Методы разрешения коллизий:
- **Метод цепочек** — каждая ячейка хранит список всех значений с одинаковым хэшем.
    принцип работы:
	- Хэшируем ключ, определяем индекс
	- Добавление элемента
		- Если ячейка пустая, создается новый список с новым элементом
		- Если нет, элемент добавляется в конец списка
	- Поиск
		- Вычисляем хэш, находим соответствующую ячейку массива
		- Производится линейный поиск по списку в этой ячейке для нахождения нужного элемента.
	- Удаление
		- Находим элемент в списке и удаляем
Недостатки: При большом количестве коллизий длина списков увеличивается что приводит ко времени поиска $O(n)$ в худшем случае. Также требуются дополнительные затраты памяти
Плюсы: Легко адаптируется под более сложные структуры данных
  
- **Открытая адресация** — поиск следующей свободной ячейки в таблице для нового элемента. 
    принцип работы:
	- Хэшируем ключ, определяем индекс
	- Добавление
		- Если ячейка пуста, просто помещаем элемент
		- Если ячейка занята используется один из способов исследования для поиска свободной ячейки:
			1. Линейное: 
			   при столкновении, ячейки проверяются последовательно, $индекс+1$ пока не найдем свободную ячейку
			2. Квадратичное: 
			   индексы вычисляются по формуле $индекс=(хэш+c_1i+c_2i^2)\modразмер$ $таблицы$, $i=1,2,3..$
			   Коэффициенты $c_1, c_2$ определяют как далеко от исходного индекса проверяются ячейки. Способ уменьшает кластеризацию, но не факт что свободная ячейка найдется (при херовой хэш-функции)
			3. Двойное хэширование:
			   Используется дополнительная хэш-функция $h_2$, которая будет определять щаг с которым проверяются индексы
			   $индекс=(хэш+ih_2(ключ))\modразмер$ $таблицы$, $i=1,2,3..$
			   Считается одним из самых эффективных методов, вероятность кластеризации минимальна
	- Поиск
		- Вычисляем хэш, находим соответствующую ячейку массива
		- Если ячейка не содержит нужный элемент, применяется та же стратегия исследования, что и при вставке, пока не будет найден элемент или пустая ячейка.
	- Удаление
		- Аналогично поиску, находятся ячейка и элемент.
		- Элемент помечается как удаленный, чтобы не нарушить работу поиска.
Недостатки: Плохо работает при высоком коэффециенте заполнения (ухудшение до  $O(n)$ в худшем случае), удаление элементов требует пометки.

>Кластеризация — ситуация, когда в хэш-таблице возникает группа подряд занятых ячеек из-за последовательных коллизий (кластер)

#### Операции хэш-таблицы
1. Вставка: 
   Вычисляется хэш ключа, значение помещается в ячейку. Если она занята, используется метод разрешения коллизий
2. Поиск:
   Вычисляется хэш ключа, и в соответствующей ячейке массива ищется нужное значение. Если используется метод цепочек, то производится поиск в списке.
3. Удаление:
   Вычисляется хэш ключа, и удаляется соответствующее значение. 
   При использовании открытой адресации значение помечается удаленным.

##### Реализация:
```python
//разрешение коллизийметодом цепочек
class HashTable:
    def __init__(self, size):
        self.size = size
        self.table = [[] for _ in range(size)]
    
    def hashFunction(self, key):
        hash_val = 0
        for ch in key:
            hash_val = (hash_val * 31 + ord(ch)) % self.size
        return hash_val
    
    def insert(self, key, value):
        index = self.hashFunction(key)
        for pair in self.table[index]:
            if pair[0] == key:
                pair[1] = value
                return
        self.table[index].append([key, value])
    
    def find(self, key):
        index = self.hashFunction(key)
        for pair in self.table[index]:
            if pair[0] == key:
                return pair[1]
        raise KeyError("Key not found")
    
    def remove(self, key):
        index = self.hashFunction(key)
        chain = self.table[index]
        for i, pair in enumerate(chain):
            if pair[0] == key:
                del chain[i]
                return
        raise KeyError("Key not found")
```

## 15. Описание структуры двоичного дерева и реализации методов обхода дерева

> Двоичное дерево — это структура данных, в которой каждый узел имеет не более двух потомков: **левого** и **правого**. Основные элементы двоичного дерева:

- **Корень (root):** вершина, с которой начинается дерево. У корня нет родителей.
- **Узел (node):** элемент дерева, который может содержать данные, а также ссылки на дочерние узлы.
- **Лист (leaf):** узел, у которого нет дочерних элементов.
- **Рёбра (edges):** соединения между узлами.
- **Высота дерева (height):** максимальная длина пути от корня до листа.

**Структура узла**
Типичная структура узла в двоичном дереве на C++ выглядит следующим образом:

```python
class Node:
    def __init__(self, value):
        self.data = value
        self.left = None
        self.right = None
```

#### Виды обхода дерева

Для работы с элементами дерева применяются методы обхода. Они делятся на два основных типа:

1.    **Обход в глубину:** исследование каждого узла дерева, двигаясь по ветвям (вертикально).
2.    **Обход в ширину:** исследование узлов уровня за уровнем (горизонтально).

##### Обходы в глубину:

1.    **Прямой (Pre-order, КЛП):**
- Посетить корень.
- Обойти левое поддерево.
- Обойти правое поддерево.

**Пример реализации:**
```python
def preOrder(node):
    if node is None:
        return
    print(node.data, end=" ")
    preOrder(node.left)
    preOrder(node.right)
```

2.    **Центрированный (In-order, ЛКП):**
- Обойти левое поддерево
- Посетить корень
- Обойти правое поддерево

**Пример реализации:**
```python
def inOrder(node):
    if node is None:
        return
    inOrder(node.left)
    print(node.data, end=" ")
    inOrder(node.right)
```

3.    **Обратный (Post-order, ЛПК):**
- Обойти левое поддерево
- Обойти правое поддерево
- Посетить корень
  
**Пример реализации:**
```python
def postOrder(node):
    if node is None:
        return
    postOrder(node.left)
    postOrder(node.right)
    print(node.data, end=" ")
```

##### Обход в ширину (по уровням, Level-order):

- Используется очередь для последовательного обхода узлов уровня за уровнем.

Пример реализации:
```python
#include <queue>

def levelOrder(root):
    if root is None:
        return
    q = []
    q.append(root)
    while q:
        current = q.pop(0)
        print(current.data, end=" ")
        if current.left is not None:
            q.append(current.left)
        if current.right is not None:
            q.append(current.right)
```


##### Применение обходов

1.    **Прямой обход:** используется для создания копий дерева, вычисления значений выражений.
2.    **Центрированный обход:** применяется для сортировки элементов, так как элементы посещаются в возрастающем порядке.
3.    **Обратный обход:** подходит для удаления дерева или вычисления выражений.
4.    **Обход в ширину:** применяется для поиска кратчайшего пути, анализа дерева по уровням.

Двоичное дерево и его обходы широко используются в информатике. Они находят применение в поиске, сортировке, представлении синтаксических выражений и других задачах.



## 16. Описание структуры двоичного дерева поиска и реализации методов добавления и удаления узла

**Двоичное дерево поиска** (Binary Search Tree, BST) — это структура данных, которая представляет собой двоичное дерево, обладающее следующими свойствами:

1.    Для любого узла значение всех ключей в левом поддереве **меньше**, чем значение ключа самого узла.
2.    Для любого узла значение всех ключей в правом поддереве **больше или равно**, чем значение ключа самого узла.
3.    Левое и правое поддеревья также являются двоичными деревьями поиска.

BST обеспечивает эффективный поиск, вставку и удаление элементов за время **O(log n)** в среднем. Однако, в случае вырожденного дерева (например, в виде цепочки) время выполнения операций может достигать **O(n)**.

**Структура узла дерева**
 ```python
class Node:
    def __init__(self, value):
        self.data = value
        self.left = None
        self.right = None
```

#### 1. Добавление узла
Метод добавления узла в двоичное дерево поиска заключается в поиске подходящего места в дереве, чтобы сохранить свойства BST. Новый элемент добавляется как лист дерева.

Алгоритм:
1.    Сначала мы **ищем место в дереве**, куда может быть вставлен новый узел, чтобы сохранились свойства BST:
- Если дерево пусто, новый узел становится корнем
- Если ключ нового узла меньше текущего ключа, ищем место в левом поддереве.
- Если ключ нового узла больше или равен текущему ключу, ищем место в правом поддереве
  
2.    При поиске используется модифицированный алгоритм поиска (`TREE-SEARCH-INEXACT`), который возвращает не только узел с искомым ключом, но и родителя последнего посещённого узла. Это нужно, чтобы правильно подключить новый узел.

3.    После нахождения подходящего узла:
- Новый узел подключается к родительскому узлу (слева или справа), в зависимости от его ключа
4.    Если узел с таким ключом уже есть в дереве, вставка не выполняется.

Пример:
Предположим, дерево содержит узлы с ключами `{10, 20, 30}`, и мы добавляем узел с ключом `32`.
- Поиск остановится на узле с ключом `30`, так как ключ `32` больше, но **правое поддерево пусто**.
- Узел с ключом `32` будет подключён к узлу `30` как правый потомок.

Итоговое время выполнения:
- $O(h)$, где $h$ — высота дерева. Это время включает поиск места для вставки и само добавление.

**Рекурсивная реализация:**

```python
def insert(root, value):
    if root is None:
        return Node(value)
    
    if value < root.data:
        root.left = insert(root.left, value)
    elif value > root.data:
        root.right = insert(root.right, value)
    
    return root
```

**Итеративная реализация:**
```python
def insertIterative(root, value):
    newNode = Node(value)
    if root is None:
        return newNode
    
    current = root
    parent = None
    
    while current is not None:
        parent = current
        if value < current.data:
            current = current.left
        else:
            current = current.right
    
    if value < parent.data:
        parent.left = newNode
    else:
        parent.right = newNode
    
    return root
```


#### 2. Удаление узла

Удаление узла из дерева требует учета трёх случаев:

1.    Узел — лист (не имеет потомков).
2.    Узел имеет одного потомка.
3.    Узел имеет двух потомков.

Алгоритм:
Удаление узла в дереве требует учёта **трёх случаев**:

1.    **Удаляемый узел не имеет потомков (лист):**
	- Узел просто удаляется
	- указатель родительского узла на этот узел становится `NULL`
2.    **Удаляемый узел имеет одного потомка:**
	- Потомок удаляемого узла подключается вместо него к его родительскому узлу
	- Это обеспечивает сохранение структуры дерева
3.    **Удаляемый узел имеет двух потомков:**
	- В этом случае на место удаляемого узла должен встать его последователь — узел с минимальным ключом в правом поддереве
	- Алгоритм:
		- Найти узел-последователь
		- Скопировать ключ узла-последователя в удаляемый узел
		- Удалить узел-последователь, используя один из первых двух случаев
		  (он гарантированно имеет не более одного потомка)

Пример:
Предположим, дерево содержит узлы с ключами `{10, 20, 30, 25}`, и мы удаляем узел с ключом `20`:
- Узел `20` имеет двух потомков (`10` и `30`).
- Его последователь — узел с ключом `25` (минимальный ключ в правом поддереве).
- Ключ `25` копируется в узел `20`.
- Узел с ключом `25` удаляется (случай с одним потомком или листом).

Итоговое время выполнения:
- $O(h)$, где $h$ — высота дерева. Это время включает поиск удаляемого узла и, при необходимости, его последователь.

**Рекурсивная реализация:**

```python
def findMin(root):
    """Находит узел с минимальным значением в дереве (самый левый узел)"""
    while root.left is not None:
        root = root.left
    return root

def remove(root, value):
    """Удаляет узел с заданным значением из бинарного дерева поиска"""
    # Базовый случай: если дерево пустое или узел не найден
    if root is None:
        return root
    
    # Поиск удаляемого узла в дереве
    if value < root.data:
        # Ищем в левом поддереве
        root.left = remove(root.left, value)
    elif value > root.data:
        # Ищем в правом поддереве
        root.right = remove(root.right, value)
    else:
        # Найден узел для удаления
        # Случай 1: узел - лист (нет потомков)
        if root.left is None and root.right is None:
            return None
        # Случай 2: узел имеет только правого потомка
        elif root.left is None:
            return root.right
        # Случай 3: узел имеет только левого потомка
        elif root.right is None:
            return root.left
        # Случай 4: узел имеет двух потомков
        else:
            # Находим минимальный узел в правом поддереве
            temp = findMin(root.right)
            # Заменяем данные текущего узла данными минимального узла
            root.data = temp.data
            # Удаляем минимальный узел из правого поддерева
            root.right = remove(root.right, temp.data)
    
    return root
```

1.    **Добавление**: Узлы добавляются рекурсивно или итеративно, соблюдая правила BST.
2.    **Удаление**: Для узла с двумя потомками требуется найти минимальный элемент в правом поддереве или максимальный в левом.
3.    **Оптимизация**: Для сохранения времени выполнения операций в **$O(\log n)$** дерево нужно сбалансировать, например, с помощью AVL-деревьев или красно-чёрных деревьев.

## 17.Описание структуры сбалансированных деревьев поиска и реализация методов малых и больших поворотов узла

#### Описание структуры сбалансированных деревьев поиска
Сбалансированное дерево поиска — это разновидность двоичного дерева поиска (Binary Search Tree, BST), которая гарантирует, что высота дерева остается примерно равной для всех узлов, что позволяет поддерживать эффективное время выполнения операций (поиск, вставка, удаление). Примеры таких деревьев: **AVL-дерево**, **красно-черное дерево**, **B-дерево** и другие.

Дерево автоматически сбалансируется при добавлении или удалении узлов, чтобы его высота оставалась $O(log n)$ где `n` — количество узлов. Для этого используются операции **поворотов узлов**.

#### Основные свойства:

1.    **Свойство BST:**
Для любого узла `x` все ключи в левом поддереве меньше ключа `x`, а все ключи в правом поддереве больше.

2.    **Балансировка:**
Разница в высоте левого и правого поддерева любого узла (баланс-фактор) не должна превышать 1:
Баланс-фактор= $h_{left}-h_{right}$ , где $|h_{left}-h_{right}| \leq 1$

#### Ротации (повороты) узлов
Повороты используются для восстановления баланса дерева после добавления или удаления узлов. Они позволяют уменьшить высоту дерева.

###### 1. Малый правый поворот
Малый правый поворот выполняется, когда баланс-фактор узла стал **2** (левое поддерево слишком высокое) и его левое поддерево имеет баланс-фактор **1** (левый подъем).
Этот поворот выполняется, если добавление узла произошло в левом поддереве левого сына опорного узла.
- **До поворота:** высота левого поддерева превышает высоту правого поддерева опорного узла на 2.
- **После поворота:** высоты поддеревьев уравниваются, и баланс восстанавливается.

**Алгоритм:**
Пусть `x` — корень поддерева, а `y` — его левый потомок:

1.    Узел `y` становится новым корнем поддерева.
2.    Правое поддерево `y` становится левым поддеревом узла `x`.
3.    Узел `x` становится правым потомком `y`.

До поворота:

```alhgoritm
	x
   / \
  y   C
 / \
A   B
```
    
```
После малого правого поворота:
	y
   / \
  A   x
     / \
    B   C
```
 

###### 2. Малый левый поворот 
Малый левый поворот выполняется, когда баланс-фактор узла стал **-2** (правое поддерево слишком высокое) и его правое поддерево имеет баланс-фактор **-1** (правый подъем).

Этот поворот выполняется, если добавление узла произошло в правом поддереве правого сына опорного узла.

- **До поворота:** высота правого поддерева превышает высоту левого поддерева опорного узла на 2.
- **После поворота:** высоты поддеревьев уравниваются.

**Алгоритм:**

Пусть `x` — корень поддерева, а `y` — его правый потомок:

1.    Узел `y` становится новым корнем поддерева.
2.    Левое поддерево `y` становится правым поддеревом узла `x`.
3.    Узел `x` становится левым потомком `y`.

До поворота:

```alhgoritm
	x
   / \
  A   y
     / \
    B   C
```

После малого левого поворота:

```alhgoritm
	y
   / \
  x   C
 / \
A   B
```


###### 3. Большой правый поворот (левый-правый)
Большой правый поворот выполняется, когда баланс-фактор узла стал **2**, но левое поддерево имеет баланс-фактор **-1** (правый подъем).

Этот поворот выполняется, если добавление узла произошло в правом поддереве правого сына опорного узла.
- **До поворота:** высота правого поддерева превышает высоту левого поддерева опорного узла на 2.
- **После поворота:** высоты поддеревьев уравниваются.

**Алгоритм:**
1.    Выполнить **малый левый поворот** для левого поддерева.
для z относительно y
2.    Затем выполнить **малый правый поворот** для опорного узла относительно нового корня.

До поворота:

```alhgoritm
	x
   / \
  y   C
 / \
A   z
   / \
  B   D
```
После большого правого поворота:

```alhgoritm
	z
   / \
  y   x
 / \ / \
A  B D  C
```

###### 4. Большой левый поворот (правый-левый)
Большой левый поворот выполняется, когда баланс-фактор узла стал **-2**, но правое поддерево имеет баланс-фактор **1** (левый подъем).

Этот поворот выполняется, если добавление узла произошло в левом поддереве правого сына опорного узла.
- **До поворота:** балансировка нарушена, и требуется два шага:
	1.    Малый правый поворот относительно левого сына правого поддерева.
	2.    Малый левый поворот относительно опорного узла.

**Алгоритм:**
1.    Выполнить **малый правый поворот** для правого поддерева.
для z относительно y
2.    Затем выполнить **малый левый поворот** для опорного узла относительно нового корня.

До поворота:

```alhgoritm
	x
   / \
  A   y
     / \
    z   C
   / \
  B   D
```

После большого левого поворота:

```alhgoritm
	z
   / \
  x   y
 / \ / \
A  B D  C
```

#### Код реализации поворотов
##### Малый правый поворот
**Код (псевдокод):**
```python
def rotateRight(root):
    """
    Выполняет правый поворот в AVL-дереве
    Используется для балансировки при перевесе в левом поддереве
    
    Args:
        root: корень поддерева для поворота
        
    Returns:
        Новый корень поддерева после поворота
    """
    # Новым корнем становится левый потомок текущего корня
    newRoot = root.left
    # Левый потомок текущего корня становится правым потомком нового корня
    root.left = newRoot.right
    # Текущий корень становится правым потомком нового корня
    newRoot.right = root
    # Возвращаем новый корень поддерева
    return newRoot
```

##### Малый левый поворот 
**Код (псевдокод):**
```python
def rotateLeft(root):
    """
    Выполняет левый поворот в AVL-дереве
    Используется для балансировки при перевесе в правом поддереве
    
    Args:
        root: корень поддерева для поворота
        
    Returns:
        Новый корень поддерева после поворота
    """
    # Новым корнем становится правый потомок текущего корня
    newRoot = root.right
    # Правый потомок текущего корня становится левым потомком нового корня
    root.right = newRoot.left
    # Текущий корень становится левым потомком нового корня
    newRoot.left = root
    # Возвращаем новый корень поддерева
    return newRoot
```


##### Большой правый поворот
**Код (псевдокод):**
```python
def bigRightRotate(x):
    """
    Выполняет большой правый поворот (левый-правый поворот) в AVL-дереве
    Используется, когда перевес в левом поддереве, но его правый потомок выше
    
    Args:
        x: корень поддерева для балансировки
        
    Returns:
        Новый корень сбалансированного поддерева
    """
    # Сначала выполняем малый левый поворот для левого потомка
    x.left = rotateLeft(x.left)
    # Затем выполняем малый правый поворот для текущего узла
    return rotateRight(x)
```
##### Большой левый поворот
**Код (псевдокод):**
```python
def bigLeftRotate(x):
    """
    Выполняет большой левый поворот (правый-левый поворот) в AVL-дереве
    Используется, когда перевес в правом поддереве, но его левый потомок выше
    
    Args:
        x: корень поддерева для балансировки
        
    Returns:
        Новый корень сбалансированного поддерева
    """
    # Сначала выполняем малый правый поворот для правого потомка
    x.right = rotateRight(x.right)
    # Затем выполняем малый левый поворот для текущего узла
    return rotateLeft(x)
```

Сбалансированные деревья поиска (например, AVL) поддерживают равномерную высоту за счёт ротаций.
- Малые повороты (правый и левый) восстанавливают баланс при одностороннем росте.
- Большие повороты (правый и левый) исправляют дисбаланс при двухстороннем росте.  
Эти операции выполняются за $O(1)$, а их вызов при необходимости обеспечивает сложность всех операций дерева на уровне **$O(\log n)$**.




## 18. Описание структуры АВЛ-дерева и метода добавление нового узла

**АВЛ-дерево** — вид бинарного дерева поиска, в котором поддерживается балансировка для обеспечения эффективности операций добавления, удаления и поиска. Основная особенность АВЛ-дерева заключается в том, что для любого узла высота его левого и правого поддеревьев может отличаться не более чем на 1.

**Основные моменты:**
1.    **Каждый узел** содержит значение, указатель на левое и правое поддеревья, а также параметр высоты.
2.    **Балансировка** необходима, чтобы дерево не становилось слишком "однобоким" (высоким с одной стороны).

---
**Алгоритм добавления нового узла:**
1.    **Поиск места для нового узла**:
	- Начинаем с корня дерева
	- Сравниваем  значение нового узла с текущим
	- Если новое значение меньше, переходим в левое поддерево, 
	    больше — в правое
	- Продолжаем, пока не находим подходящее место (пустую позицию)
	  
2.    **Добавление узла**
- Вставляем новый узел на найденное место
  
3.    **Обновление высоты**:
- После добавления узла обновляем высоты всех узлов на пути вверх (от нового узла к корню)

4.    **Проверка баланса**:
- Для каждого узла вычисляем разницу высот между левым и правым поддеревом (баланс-фактор)
- Если разница превышает $|1|$, дерево разбалансировано

5.    **Восстановление баланса**:
- Выполняем один из четырех поворотов для сбалансирования дерева

## 19. Описание структуры АВЛ-дерева и метода удаления существующего узла

в предыдущем посте

---
**Алгоритм удаления узла:**

1.    **Поиск узла для удаления**:
- Идем от корня, сравнивая значение искомого узла с текущими узлами, пока не найдем нужный

2.    **Удаление узла**:
- Если у узла нет потомков, просто удаляем его
- Если есть только один потомок, заменяем узел на этого потомка
- Если два потомка, находим минимальный узел в правом поддереве (or max in left), копируем его значение в текущий узел и удаляем найденный минимальный узел.

3.    **Обновление высоты**:
- После удаления обновляем высоты всех узлов на пути вверх
  
4.    **Проверка баланса**
- Для каждого узла проверяем баланс-фактор
- Если разница высот превышает $|1|$, дерево разбалансировано

5.    **Восстановление баланса**:
- Применяем соответствующие повороты

## 20. Описание структуры Красно-Черного дерева и метода добавление нового узла

#### КЧ-деревья
Это самобалансирующиеся двоичные деревья поиска, у которого каждый узел хранит дополнительное поле с цветом
**Преимущества**: 
- Баланс достигается за счет ограниченного числа операций – не более 2х поворотов и 1 перекрашивания
- Логарифмическая сложность операций 

Структура узла:
```python
class RBNode:
    """
    Узел красно-черного дерева
    """
    def __init__(self, key):
        self.key = key          # Ключ узла
        self.left = None       # Левый потомок
        self.right = None      # Правый потомок
        self.parent = None     # Родительский узел
        self.color = 'R'       # Цвет узла: 'R' (красный) или 'B' (черный)
```
Если указатели $left$, $right$ равны нулю, то они будут являться указателями на **фиктивные листья**. Таким образом все узлы - внутренние, нелистовые.

**Свойства:** 
- Каждый узел имеет цвет
- Корень дерева всегда черный
- Каждый лист (фиктивный) – черный
- Если узел красный, оба потомка – черные
- Все пути, идущие от корня к любому фиктивному листу, содержат одинаковое количество черных узлов (черная высота)
- Операции поиск, вставки и удаления занимают $O(\log n)$

> Черная высота – количество черных узлов на пути от этого узла, у узлу у которого оба сына - фиктивные листья. Сам узел не включается в это число
									 *черная высота дерева - черная высота корня*

---
#### Вставка узла
 
**Описание алгоритма:** 
1. Вставка идет по правилам двоичного дерева поиска
	- Вновь добавленный узел становится красным
	- Если это первый узел – перекрашивается в черный
2. Проверка свойств дерева 
	- Если родитель нового узла черный все в порядке
	- Если родитель нового узла красный, это нарушение свойств дерева, требующее коррекции
Для коррекции используется перекраска и повороты:

#####  Возможные случаи для исправления
Обозначения: 
Добавленный узел $X$ |  Его родитель (отец) $F$ |  Дед  $G$  | Дядя (второй сын деда) $U$ |
Поддеревья обозначаются как $Ti$ 

Итак, добавили новый узел
Узел $X$ и его отец - красные, соответственно дед - черный. Дядя может иметь любой цвет.

##### Случай 1: Дядя $U$ узла $X$ – **красный**
Достаточно перекраски отца и дяди (узлы $F$ и $U$) в черный цвет, а деда $G$ в красный.
- Если $X$ - корень, он просто перекрашивается в черный
- Если у $G$ родитель тоже красный, образуется нарушение и цикл повторяется, узел $G$ становится новым $X$
![[Pasted image 20250119074146.png]]
##### Случай 2: Дядя $U$ – **черный** и цепочка $X$-$F$-$G$ образует прямую
Перекраски $F$ и $G$ не хватит, потому что количество черных узлов в путях, проходящих через G направо, сократится на 1. Поэтому добавляем одинарный поворот $G$ относительно $F$ 

- Перекраска отца $F$ в черный, деда $G$ в красный
- $X$ находится в *левом* поддереве $F$, значит в данном случае поворот *вправо* относительно $F$
![[Pasted image 20250119074204.png]]
Черная высота осталась прежней, условия соблюдены

##### Случай 3: Дядя $U$ – **черный** и цепочка $X$-$F$-$G$ образует угол
Перекрашивая $X$ в черный, $F$ в красный, получаем новое красное нарушение, разрешающееся двойным поворотом комбинации $X$-$F$-$G$ (нижний узел окажется сверху)
```markdown
    G              G
   /              /
  F              X          X
   \            /          / \
    X          F          F   G
 
```
- Перекраска $X$ - черный, $X$ - красный
- Двойной поворот 

![[Pasted image 20250119074659.png]]

#### Реализация
```python
def RBInsert(T, x):
    """
    Вставка узла в красно-черное дерево
    """
    # 1. Стандартная вставка как в бинарном дереве поиска
    TreeInsert(T, x)
    
    # 2. Новый узел всегда красится в красный цвет
    x.color = 'R'
    
    # 3. Исправление нарушений свойств красно-черного дерева
    while x != T.root and x.parent.color == 'R':
        if x.parent == x.parent.parent.left:  # Родитель - левый потомок
            y = x.parent.parent.right  # Дядя узла x
            
            if y.color == 'R':  # Случай 1: дядя красный
                x.parent.color = 'B'
                y.color = 'B'
                x.parent.parent.color = 'R'
                x = x.parent.parent
            else:  # Дядя черный
                if x == x.parent.right:  # Случай 3: x - правый потомок
                    x = x.parent
                    LeftRotate(T, x)
                
                # Случай 2: x - левый потомок
                x.parent.color = 'B'
                x.parent.parent.color = 'R'
                RightRotate(T, x.parent.parent)
        else:  # Родитель - правый потомок (симметричный случай)
            y = x.parent.parent.left  # Дядя узла x
            
            if y.color == 'R':  # Случай 1: дядя красный
                x.parent.color = 'B'
                y.color = 'B'
                x.parent.parent.color = 'R'
                x = x.parent.parent
            else:  # Дядя черный
                if x == x.parent.left:  # Случай 3: x - левый потомок
                    x = x.parent
                    RightRotate(T, x)
                
                # Случай 2: x - правый потомок
                x.parent.color = 'B'
                x.parent.parent.color = 'R'
                LeftRotate(T, x.parent.parent)
    
    # Корень всегда должен быть черным
    T.root.color = 'B'
```





## 21. Описание структуры Красно-Черного дерева и метода удаления существующего узла

Структура в предыдущем пункте
Свойства
- Каждый узел либо красный, либо черный.
- Корень всегда черный.
- Все листья (фиктивные узлы) черные.
- Если узел красный, то оба его потомка черные (два подряд красных нельзя).
- На любом пути от корня до листа или фиктивного узла количество черных узлов одинаково.
#### Удаление существующего узла
**Описание алгоритма:** 
1. Удаление с по правилам двоичного дерева поиска (зависящего от количества потомков)
	- 2 потомка узла - удаляемым узлом становится его последователь (самый левый элемент правого поддерева). Таким образом у удаляемого узла будет максимум один сын (правый). Это и предполагается в дальнейшем
2. Проверка и восстановление свойств дерева 
	- Если удаляемый узел - **красный**, все в порядке
	- Если удаляемый узел - **черный**, могут потребоваться корректировки
		- Если удаляемый узел имеет два сына - его заменяет наименьший потомок в правом поддереве (или наоборот наибольший в левом)
		- В случае если у удаляемого узла есть красный сын, будет достаточно перекрасить сына в черный цвет
		- В случае черного сына рассмотрим 5 ситуаций
Для коррекции используется перекраска и повороты:

Обозначения: 
Сын удаленного узла  $N$ | Отец удаленного $F$ | Новый брат $B$ |
(после удаления, F стал новым отцом N)
 $CL$ и $CR$  левый и правый сыновья $B$
Поддеревья обозначаются как $Ti$ 
Белый цвет узла означает любой цвет

Рассматриваем случай:
$N$ - левый сын $F$ 
Удаляем черный узел, имеющий черного сына N

##### Случай 1: Отец F узла N красный, остальные узлы черные
Достаточно перекрасить узлы F и B, после этого количество черных узлов в путях, идущих через F налево увеличится на 1, а направо не изменится. Значит все пути все еще содержат одинаковое количество черных узлов
![[Pasted image 20250120043538.png]]

##### Случай 2: Брат B узла N черный, а его правый сын CR красный
Независимо от цвета F, свойства КЧ-дерева восстанавливаются после поворота F относительно B и перекраски CR в черный цвет, а также обмена цветами F и B.
![[Pasted image 20250120045027.png]]
Почему? Если F был черным, то и B станет черным, поэтому после поворота количество черных узлов слева увеличится на 1, справа же не изменится тк CR теперь черный.
Если же F был красным то количество черных узлов
слева все равно увеличится на один, потому что после поворота F
перекрашивается в черный цвет. А справа количество черных уз-
лов не изменится, потому что B станет красным

##### Случай 3: Брат B узла N черный, его правый сын CR черный, а левый сын CL красный
Этот случай сводится к предыдущему путем поворота CL относительно F, выстраивая правое поддерево в линию
Далее перекрашиваем CL и B
![[Pasted image 20250120045120.png]]

##### Случай 4: Брат B узла N красный
 F, CL и CR могут быть только черными. потребуется несколько действий
 - Левый поворот F относительно B 
 - Перекраска F и B
 Тогда все сводится к первым 3м случаям, когда брат узла N - черный
![[Pasted image 20250120045133.png]]

##### Случай 5: Все узлы комбинации черные
Достаточно перекраски B в красный цвет, для восстановления черной высоты
После этого:
-Количество черных узлов в пути от удаленного узла через правую часть уменьшится на один черный узел (вместо черного узла будет красный)
-Если F имеет предков, нужно будет продолжить восстановление свойств КЧ-дерева наверх, рассматривая F как узел N и проверяя все случаи
![[Pasted image 20250120045157.png]]


##### Вкратце:

| №   | Ситуация                        | Действие                                                                           |
| --- | ------------------------------- | ---------------------------------------------------------------------------------- |
| 1   | F красный, остальные черные     | перекрасить узлы F в чёрный, B в красный.                                          |
| 2   | B чёрный, CR красный            | левый поворот F относительно B, <br>перекраска CR в чёрный,<br>F и B меняют цвета. |
| 3   | B чёрный, CR чёрный, CL красный | правый поворот CL относительно F, <br>перекраска CL и B, <br>переход к случаю 2.   |
| 4   | B красный                       | левый поворот F относительно B, <br>перекраска F и B, <br>переход к случаю 1-3.    |
| 5   | все чёрные                      | перекраска B в красный, <br>переход к следующему узлу (при необходимости)          |

##### Реализация:
удаление с использованием вспомогательной процедуры восстановления свойств
```python
def RB_DELETE_FIXUP(T, n):
    """
    Восстановление свойств красно-черного дерева после удаления узла
    
    Args:
        T: красно-черное дерево
        n: сын удаленного узла, от которого начинается восстановление
    """
    while n != T.root and n.color == 'B':
        if n == n.parent.left:  # n - левый потомок
            b = n.parent.right  # b - брат n
            
            # Случай 4: брат красный
            if b.color == 'R':
                b.color = 'B'
                n.parent.color = 'R'
                TREE_ROTATE_L(T, n.parent)
                b = n.parent.right  # теперь у n черный брат
            
            # Случай 1 или 5: оба потомка брата черные
            if b.left.color == 'B' and b.right.color == 'B':
                b.color = 'R'
                n = n.parent
            else:
                # Случай 3: правый потомок брата черный
                if b.right.color == 'B':
                    b.left.color = 'B'
                    b.color = 'R'
                    RIGHT_ROTATE(T, b)
                    b = n.parent.right
                
                # Случай 2
                b.color = n.parent.color
                n.parent.color = 'B'
                b.right.color = 'B'
                LEFT_ROTATE(T, n.parent)
                n = T.root  # выход из цикла на следующей итерации
        else:
            # Симметричный фрагмент для n - правого потомка
            b = n.parent.left  # b - брат n
            
            # Случай 4: брат красный
            if b.color == 'R':
                b.color = 'B'
                n.parent.color = 'R'
                TREE_ROTATE_R(T, n.parent)
                b = n.parent.left  # теперь у n черный брат
            
            # Случай 1 или 5: оба потомка брата черные
            if b.right.color == 'B' and b.left.color == 'B':
                b.color = 'R'
                n = n.parent
            else:
                # Случай 3: левый потомок брата черный
                if b.left.color == 'B':
                    b.right.color = 'B'
                    b.color = 'R'
                    LEFT_ROTATE(T, b)
                    b = n.parent.left
                
                # Случай 2
                b.color = n.parent.color
                n.parent.color = 'B'
                b.left.color = 'B'
                RIGHT_ROTATE(T, n.parent)
                n = T.root  # выход из цикла на следующей итерации
    
    n.color = 'B'


def RB_DELETE(T, z):
    """
    Удаление узла из красно-черного дерева
    
    Args:
        T: красно-черное дерево
        z: узел для удаления
        
    Returns:
        Удаленный узел
    """
    # Определяем узел y для удаления
    if z.left is None or z.right is None:
        y = z
    else:
        y = TREE_SUCCESSOR(z)
    
    # Определяем потомка x узла y
    if y.left is not None:
        x = y.left
    else:
        x = y.right
    
    # Обновляем ссылки родителя
    if x is not None:
        x.parent = y.parent
    
    # Обновляем корень или ссылку родителя
    if y.parent is None:
        T.root = x
    else:
        if y == y.parent.left:
            y.parent.left = x
        else:
            y.parent.right = x
    
    # Если y не равен z, копируем ключ
    if y != z:
        z.key = y.key
    
    # Если удаленный узел черный, восстанавливаем свойства
    if y.color == 'B' and x is not None:
        RB_DELETE_FIXUP(T, x)
    
    return y

```

## 22. Описание структуры B-дерева и реализация алгоритма поиска ключа

 [Материалы на сайте Пелевина](https://markoutte.me/students/b-tree/)

>B-дерево — это сбалансированное дерево поиска, которое удволетворяет следующим условиям:
1. Упорядоченность:
	1. Ключи в каждом узле хранятся в порядке "не убывания" (по возрастанию, но допускается $k_1 \leq k_2$).
	2. Так как В-дерево — это двоичное дерево поиска, то все элементы в левом поддереве меньше, а в правом больше.
2. Ограниченное количество ключей:
	1. Каждый узел может содержать от $t - 1$ до $2t-1$ ключей, где $t$ - порядок дерева (величина задающаяся при создании дерева и $t \geq 2$)
	2. Корень может содержать от $1$ до $2t-1$ ключей (пустое дерево исключение)
3. Ограниченное количество детей:
	1. Узел может содержать от $t$ до $2t$ дочерних узлов
4. Специальная структура узла. Каждый узл состоит из:
	1. Ключей ($k_1,k_2,...,k_i$) в порядке не убывания
	2. Ссылок на дочерние узлы ($C_1,C_2,...,C_i$)
	3. Данные связаные с ключём

##### Пример B-дерева:
![[image144.gif]]

#### Как работает поиск ключа. 
1. Смотрим какой-то конкретный узел
2. Используя линейный поиск, ищем наименьший индекс $i$ ключа для которого справедливо, что $k \leqslant x.key_i$. То есть ищём первый попавшийся ключ, который численно больше или равен ключу искомого значения $x$
3. Если $k = x.key_i$, то искомый элемент найден и алгоритм заканчивает свою работу.  
4. Если узел листовой, то элемент не найден и алгоритм заканчивает работу. В противном случае загрузить блок по адресу $c.key_i$ и перейти к п. 2.
![[Screenshot-2020-11-30-at-17.34.00.png]]
Пример задействованных узлов (светло-серого цвета), при поиске элемента $R$

## 23. Описание метода добавления нового ключа в B-дерева

 [Материалы на сайте Пелевина](https://markoutte.me/students/b-tree/)

> [!Алгоритм добавления нового ключа]
> 1. Ищем нужный ключ в дереве
> 2. Если мы его не находим и текущий узел — лист, то добавляем ключ, сохраняя порядов "не убывания"
> 3. Если после добавления узла у нас происходит переполнение, то мы делим узел пополам, а тот узел, который будет являтся медианой уходит в родительский узел и начитает ссылаться на две половинки, которые мы создали.
 >
 Если же после добавления медианы в родительский улез и там случается переполнение, то алгоритм случается заново.
 Если же так доходим до корня, то корень тоже делим пополам, а медиана становится новым корнем.

#### Пример добавления узла $R$.
1. Искали узел — не нашли
2. Добавили $R$ в нужном порядке
3. Случилось переполнение
4. Поделили узел пополам, а медиану записали в родительский узел
![[Screenshot-2020-12-01-at-11.14.29.png]]


## 24. Описание метода удаления существующего ключа из B-дерева

 [Материалы на сайте Пелевина](https://markoutte.me/students/b-tree/)
#### Изначальное дерево
![[Screenshot-2020-12-07-at-16.02.46.png]]

Для удаления элемента есть 3 случая:
1. Если ключ k находится в листе (узел x), то мы просто удаляем ключ
   ![[Screenshot-2020-12-07-at-16.07.21.png]]
2. Если ключ находится в промежуточном узле:
	1. Если у родителя узла, где находится ключ $k$ есть как минимум $t$ ключей, то надо найти в поддереве этого родителя элемент $k'$, предшествующий $k$ (к примеру надо удалить 20, предшествующий элемент 15, если после никого нет). Рекурсивно удалить $k'$ и заменить $k$ на $k'$ в узле $x$	   ![[Screenshot-2020-12-07-at-16.07.35 1.png]]
	   В данном примере удалили $M$. Предшествующий ей элемент — $L$ был поставлен там, где была $M$
	   2. Eсли в родителя меньше $t$ ключей, то проверить дочерний элемент $z$, который следует за $k$  в узле $x$. Выполнить аналогичную **случаю 2.1** проверку ключа, который идёт следующий по порядку после ключа $x$. 
	   3. Если оба потомка $y$ и $z$ (левый и правый от родителя) содержат только $t - 1$ ключ, то добавить $k$ и все ключи из $z$ (правого) в $y$ (левый), после чего из $x$ пропадают $k$ и ссылка на $z$ (остаётся ссылка на элемент $x$, который представляет собой теперь $y+z$), а $y$ содержит теперь ровно $2t - 1$ ключей. После чего удалить пустой узел $z$ и рекурсивно удалить $k$ из $y$.![[Screenshot-2020-12-07-at-16.08.01.png]]
3.   
    При отсутствии ключа в промежуточных узлах ищем поддерево. Если поддерево содержит только t-1 ключей, но выполнить шаги 1 или 2 для гарантии перехода к узлу содержащему минимум t ключей. Затем рекурсивно удаляем ключ из дочернего узла
	1. Если поддерево содержит t-1 ключ, но **имеет** непосредственного соседа с не менее чем t ключами:
	   Добавить в поддерево дополнительный ключ путем переноса ключа из x вниз, а из соседского уза (справа или слева) перенесли ключ в x и обновить ссылку ![[Pasted_image_20250121074406.png]]
	   Тут мы удалили узел B: ключ C опустился влево, а ключ E поднимается выше
	2. Если поддерево и оба непосредственных соседа содержат t-1 ключ, то следует объединить поддерево с кем-то из дочерних узлов, что влечет перемещение ключа из x вниз, в новый объединенный узел и делает его медианным ключом для этого узла ![[Pasted_image_20250121074630.png]]
	   Рекурсия не может спустится к узлу CL поскольку он имеет только 2 ключа, поэтому мы перемещаем P ниже и объединяем с CL и TX, после чего удаляем узел D из листового узла. После чего высота дерева уменьшается на 1.
	   
	   Этот случай необходим, чтобы при рекурсивном обходе B-дерева от корня к искомому элементу, на каждом у нас была возможность забрать элемент из родительского узла для слияния

## 25. Описание и реализация алгоритма пузырьковой сортировки (bubble)

Алгоритм пузырьковой сортировки (bubble sort) — это простой метод сортировки, который работает путём многократного прохода по списку, сравнивая пары соседних элементов и меняя их местами, если они расположены в неправильном порядке. Этот процесс продолжается до тех пор, пока массив не будет полностью отсортирован.

**Описание алгоритма**
1.    Начать с первого элемента массива.
2.    Сравнить текущий элемент с его соседом справа.
3.    Если текущий элемент больше (или меньше, в случае сортировки по убыванию) соседнего, поменять их местами.
4.    Перейти к следующей паре элементов и повторить шаги 2–3.
5.    Когда проход по массиву завершён, самый "тяжёлый" элемент оказывается в конце массива.
6.    Повторить шаги 1–5 для оставшейся неотсортированной части массива.
7.    Процесс завершается, когда при проходе не происходит ни одной перестановки.

**Характеристики алгоритма**
- **Сложность в худшем случае**: O(n^2) (квадратичная).
- **Сложность в лучшем случае**: O(n) (если массив уже отсортирован).
- **Сложность по памяти**: O(1) (работает на месте, без дополнительных структур).

**Реализация**:
```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        isswapped = False
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                isswapped = True
        if not isswapped:
            break
    return arr
```
## 26. Описание и реализация алгоритма сортировки вставками (insertion)
#### Сортировка вставками
На каждом шаге из набора входных данных выбирается элемент и помещается в нужное место в уже отсортированной последовательности; шаги продолжаются до тех пор, пока набор входных данных не закончится.

Временная сложность: Худшая: O(n^2), Средняя: O(n^2), Лучшая: O(n).

```python
def insertionSort(arr):
    n = len(arr)
    for i in range(1, n):
        key = arr[i]
        j = i - 1
        while j >= 0 and arr[j] > key:
            arr[j + 1] = arr[j]
            j = j - 1
        arr[j + 1] = key
    return arr
```

## 27.Описание и реализация алгоритма селекционной сортировки (selection).

### Селекционная сортировка
Простой алгоритм сортировки, который работает последовательно находя минимальный (max) элемент в неотсортированной части массива и перемещая его в отсортированную часть.
Алгоритм сравнения на месте, в котором список делится на 2 части: отсортированную в левом конце и неотсортированную в правом.
Изначально отсортированная часть пуста, а несортированная - весь список.
Из несортированного массива выбирается наименьший элемент и меняется местами с крайним левым элементом, после чего этот элемент становится частью отсортированного массива. Этот процесс продолжается, перемещая границу несортированного массива на один элемент вправо.

Сложность: | Худшее время: $O(n^2)$ | Лучшее время: $O(n)$ | В среднем: $O(n^2)$
Из-за этого не подходит для больших наборов данных.

Сортировка по выбору обычно используется, когда:

- Необходимо отсортировать небольшой массив
- Стоимость замены не имеет значения
- Обязательна проверка всех элементов

**Основная суть:**
1.    Разделить массив на две части:
	- Отсортированная часть (слева);
	- Неотсортированная часть (справа).
2.    На каждой итерации:
	- Найти минимальный (или максимальный) элемент в неотсортированной части массива;
	- Обменять его с первым элементом неотсортированной части массива.
3.    Повторять процесс до тех пор, пока массив не будет отсортирован.

**Пошаговый пример работы:**

Для массива $[64,25,12,22,11]$:
1.    Найти минимальный элемент (11) и обменять его с первым (64): $[11,25,12,22,64]$
2.    Найти минимальный элемент в оставшейся части (12) и обменять его с 25: $[11,12,25,22,64]$
3.    Найти следующий минимальный элемент (22) и обменять его с 25: $[11,12,22,25,64]$
4.    Повторить для оставшихся элементов, если нужно

**Итоговый массив**: $[11,12,22,25,64]$

**Псевдокод**:
```pseudo
# Псевдокод 1: Сортировка вставками (Insertion Sort)
def insertion_sort(A):
    # идем со второго элемента (индекс 1)
    for j in range(1, len(A)):
        value = A[j]
        i = j - 1  # идем с первого элемента
        while i >= 0 and A[i] > value:
            A[i + 1] = A[i]
            i = i - 1
        A[i + 1] = value
    return A
```

```pseudo2
# Псевдокод 2: Сортировка выбором (Selection Sort)
def selection_sort(array):
    n = len(array)
    for i in range(n):
        min_index = i
        for j in range(i + 1, n):
            if array[j] < array[min_index]:
                min_index = j
        array[i], array[min_index] = array[min_index], array[i]
    return array
```

1.    Внешний цикл проходит по каждому элементу массива, начиная с первого.
2.    Внутренний цикл ищет индекс минимального элемента в неотсортированной части массива.
3.    После завершения внутреннего цикла минимальный элемент обменивается с текущим элементом, чтобы переместить его в отсортированную часть.
##### Сложность
**Временная сложность:**
- В худшем и среднем случае: $O(n^2)$, так как требуется два вложенных цикла. 
	 - **Наихудшая сложность:** возникает, когда требуется отсортировать элементы массива в обратном порядке. Это означает, что, например, вам нужно отсортировать элементы массива в порядке возрастания, но они расположены в порядке убывания. 
     Наихудшая временная сложность сортировки выбором составляет $O(n^2)$

	- **Средняя сложность в общем случае:** возникает, когда элементы массива расположены в произвольном порядке, не по возрастанию и не по убыванию. Средняя временная сложность сортировки выбором составляет O(n2).

- **В лучшем случае** (отсортированный массив): всё равно O(n2), так как нет раннего выхода. Возникает, когда сортировка не требуется, то есть массив уже отсортирован. 

 **Пространственная сложность**: 
	 $O(1)$, так как сортировка выполняется "на месте", без использования дополнительной памяти.

##### Преимущества и недостатки:
**Преимущества**:
- Простой для понимания и реализации.
- Подходит для небольших массивов.
- Устойчивая (алгоритм, который сохраняет относительный порядок элементов с одинаковыми значениями)
	  Допустим, у нас есть массив объектов:
	```css
	[("A", 3), ("B", 2), ("C", 3), ("D", 1)]
	```
	 Устойчивая сортировка: 
	```css
	[("D", 1), ("B", 2), ("A", 3), ("C", 3)]
	```
	

**Недостатки**:
- Низкая эффективность на больших массивах.
- Не является стабильным (элементы с одинаковыми ключами могут менять порядок).

## 28. Описание и реализация алгоритма сортировки слияниями (merge-sort)
Сортировка слиянием является эффективной сортировкой т.к. во всех случаях она работает за $O(n \lg n)$

Худшее время: $O(n \lg n)$ | Лучшее время: $O(n \lg n)$ | В среднем: $O(n \lg n)$ | Требует памяти: $O(n)$

> [!Алгоритм сортировки]
> 1. Исходный массив делится пополам до тех пор, пока не останутся массивы длинной 1
> 2. Далее эти массивы поочерёдно сортируются склеиваясь в один массив. Например у нас есть массив 5 и 4. Мы сравниваем их и склеиваем в один массив в правильном порядке (по возрастанию или убыванию). К примеру в $[5,4]$. 
> 3. Таким же образом мы склеиваем и сортируем массивы уже длинны 2, 3 и т.д

Реализация на C++

```python
def merge(vec, left, mid, right):
    """
    Слияние двух отсортированных подмассивов
    """
    left_size = mid - left + 1
    right_size = right - mid
    
    # Создаем временные массивы
    left_vec = [0] * left_size
    right_vec = [0] * right_size
    
    # Копируем данные во временные массивы
    for i in range(left_size):
        left_vec[i] = vec[left + i]
    for j in range(right_size):
        right_vec[j] = vec[mid + 1 + j]
    
    # Слияние временных массивов обратно в vec[left..right]
    i = 0  # Индекс для left_vec
    j = 0  # Индекс для right_vec
    k = left  # Индекс для vec
    
    while i < left_size and j < right_size:
        if left_vec[i] <= right_vec[j]:
            vec[k] = left_vec[i]
            i += 1
        else:
            vec[k] = right_vec[j]
            j += 1
        k += 1
    
    # Копируем оставшиеся элементы left_vec, если есть
    while i < left_size:
        vec[k] = left_vec[i]
        i += 1
        k += 1
    
    # Копируем оставшиеся элементы right_vec, если есть
    while j < right_size:
        vec[k] = right_vec[j]
        j += 1
        k += 1


def merge_sort(vec, left, right):
    """
    Сортировка слиянием
    """
    if left < right:
        # Вычисляем середину
        mid = left + (right - left) // 2
        
        # Сортируем первую и вторую половины
        merge_sort(vec, left, mid)
        merge_sort(vec, mid + 1, right)
        
        # Сливаем отсортированные половины
        merge(vec, left, mid, right)
```



## 29. Описание и реализация алгоритма быстрой сортировки (quick-sort)

Является эффективной сортировкой использующей метод разделяй и властвуй.

Худшее время: $O(n^2)$  | Лучшее время: $O(n \lg n)$ | В среднем: $O(n \lg n)$
Не использует дополнительную память
Худший случай возникает в случае выбора в качестве опорного элемента максимального или минимального значения в массиве (например в отсортированном массиве). 

**Основной алгоритм работы:**
1. Выбрать опорный элемент (pivot).
2. Разделение: массив разделяется на два подмассива, элементы одного из которых не превышают опорного элемента, а элементы другого — больше. После разделения опорный элемент будет находиться на своём месте.
3. Рекурсивно повторить предыдущие два шага обеих образовавшихся подпоследовательностей («слева» и «справа» от выбранного опорного элемента).
4. Рекурсивная сортировка подмассивов


**void quicksort()** - рекурсивная функция, которая вызывает саму себя для двух подмассивов — слева и справа от опорного элемента, после того как массив был разделён
**int partition()** - функция принимает массив и разделяет его на две части относительно опорного элемента. Все элементы, меньшие или равные опорному, перемещаются в левую часть массива, а все элементы большие опорного — в правую. Опорный элемент ставится на свою окончательную позицию.
Реализация
```python
def quicksort(A, p, r):
    """
    Рекурсивная функция быстрой сортировки
    
    Args:
        A: массив для сортировки
        p: начальный индекс подмассива
        r: конечный индекс подмассива
    """
    if p < r:
        q = partition(A, p, r)  # Разбиение массива
        quicksort(A, p, q - 1)  # Сортировка левой части
        quicksort(A, q + 1, r)  # Сортировка правой части


def partition(A, p, r):
    """
    Функция для разделения массива на две части
    
    Args:
        A: массив для разделения
        p: начальный индекс подмассива
        r: конечный индекс подмассива
        
    Returns:
        Индекс опорного элемента после разделения
    """
    x = A[r]  # Опорный элемент (последний элемент)
    i = p - 1  # Индекс для меньших элементов
    
    for j in range(p, r):
        if A[j] <= x:
            i += 1
            A[i], A[j] = A[j], A[i]  # Меняем местами A[i] и A[j]
    
    A[i + 1], A[r] = A[r], A[i + 1]  # Перемещаем опорный элемент на правильную позицию
    return i + 1  # Возвращаем индекс опорного элемента
```





## 30. Описание и реализация алгоритма пирамидальной сортировки (heap-sort)

#### Описание пирамидальной сортировки (Heap Sort)
Пирамидальная сортировка (heap-sort) — эффективный алгоритм сортировки, который использует структуру данных, называемую **кучей**. 
Этот метод позволяет отсортировать массив, организуя его элементы в виде **максимальной кучи** (или минимальной, в зависимости от задачи), а затем постепенно извлекая элементы из нее.

**Основные термины:**
1. **Куча (Heap)**:
	- Куча — это двоичное дерево, которое удовлетворяет свойству кучи:
		- Для **максимальной** кучи: каждый узел дерева больше (или равен) своих потомков
		- Для **минимальной** кучи: каждый узел меньше (или равен) своим потомкам
	- Куча обычно реализуется на основе массива, где:
		- Индекс узла i:
			- Левый потомок: 2i + 1
			- Правый потомок: 2i + 2
			- Родитель: (i - 1) // 2
2. **Максимальная куча**
	- Дерево, в котором самый большой элемент находится в корне (первый элемент массива)
3.    **Пирамидальная структура массива**:
	- Представление массива в виде дерева, чтобы работать с ним как с кучей
4.    **Просеивание (Heapify)**:
	- Процесс восстановления свойства кучи для узла и его потомков. Используется, если свойство кучи нарушено

**Алгоритм пирамидальной сортировки:**
1.    **Построение максимальной кучи**:
	- Преобразуем исходный массив в максимальную кучу
	- Для этого начинаем с последнего ненулевого узла, двигаясь к корню, и применяет процедуру Heapify
2.    **Извлечение максимального элемента**:
	- После построения кучи самый большой элемент находится в корне (первый элемент массива)
	- Меняем корневой элемент с последним элементом массива
	- Считаем последний элемент отсортированным и больше не включаем его в кучу
3.    **Восстановление свойства кучи**:
	- Применяем процедуру Heapify для корня (первого элемента), чтобы восстановить свойства кучи для оставшихся элементов
4.    **Повторение**:
	- Повторяем шаги 2 и 3, пока не останется элементов в куче\

#### Реализация
```python
def heapify(arr, n, i):
    largest = i  # Предполагаем, что текущий узел - наибольший
    left = 2 * i + 1  # Левый потомок
    right = 2 * i + 2  # Правый потомок
    
    # Если левый потомок больше текущего узла
    if left < n and arr[left] > arr[largest]:
        largest = left
    
    # Если правый потомок больше текущего узла
    if right < n and arr[right] > arr[largest]:
        largest = right
    
    # Если наибольший элемент не текущий узел
    if largest != i:
        arr[i], arr[largest] = arr[largest], arr[i]  # Меняем местами
        heapify(arr, n, largest)  # Рекурсивно вызываем heapify для поддерева


def heap_sort(arr):
    n = len(arr)
    
    # Построение максимальной кучи
    for i in range(n // 2 - 1, -1, -1):
        heapify(arr, n, i)
    
    # Извлечение элементов из кучи
    for i in range(n - 1, 0, -1):
        arr[0], arr[i] = arr[i], arr[0]  # Перемещаем корень в конец массива
        heapify(arr, i, 0)  # Восстанавливаем свойство кучи для оставшейся части
```

## 31. Описание и реализация алгоритма поразрядных, блочных сортировок и сортировки подсчётом

#### Поразрядная сортировка 
Aлгоритм сортировки, который выполняется за линейное время. Сравнение производится поразрядно: сначала сравниваются значения одного крайнего разряда, и элементы группируются по результатам этого сравнения, затем сравниваются значения следующего разряда, и так далее. (d – кол-во разрядов, k – система счисления)

Худшая: O(d*(n + k)), Средняя: O(d*(n + k)), Лучшая: O(d*(n + k)).  
```python
def radixSort(arr):
    n = len(arr)
    m = getMax(arr, n)
    
    exp = 1
    while m // exp > 0:
        countSort(arr, n, exp)
        exp *= 10
```

#### Сортировка подсчётом

Худшая: O(n+k), Средняя: O(n+k), Лучшая: O(n+k).

```python
def countSort(arr, n, exp):
    output = [0] * n
    count = [0] * 10
    
    # Подсчитываем количество цифр в разряде exp
    for i in range(n):
        count[(arr[i] // exp) % 10] += 1
    
    # Преобразуем count в префиксную сумму
    for i in range(1, 10):
        count[i] += count[i - 1]
    
    # Заполняем выходной массив в обратном порядке для стабильности
    for i in range(n - 1, -1, -1):
        digit = (arr[i] // exp) % 10
        output[count[digit] - 1] = arr[i]
        count[digit] -= 1
    
    # Копируем отсортированные данные обратно в arr
    for i in range(n):
        arr[i] = output[i]
```

#### Блочная сортировка
Aлгоритм сортировки, который сортирует массив путём разделения его на блоки фиксированного размера, сортировки каждого блока по отдельности, а затем объединения отсортированных блоков обратно в единый отсортированный массив.

Худшая: O(n^2), Средняя: O(n+k), Лучшая: O(n+k). k – среднее число элементов в блоке

```python
def insertionSort(bucket):
    for i in range(1, len(bucket)):
        key = bucket[i]
        j = i - 1
        while j >= 0 and bucket[j] > key:
            bucket[j + 1] = bucket[j]
            j -= 1
        bucket[j + 1] = key
    return bucket

def bucketSort(arr, n):
    b = [[] for _ in range(n)]
    
    for i in range(n):
        bi = int(n * arr[i])
        b[bi].append(arr[i])
    
    for i in range(n):
        insertionSort(b[i])
    
    index = 0
    for i in range(n):
        for j in range(len(b[i])):
            arr[index] = b[i][j]
            index += 1
```


## 32. Описания способов хранения графов, а также реализация поиска в ширину и алгоритма поиска кратчайшего пути в незвзвешанном графе.

#### Способы хранения графов
###### 1.    **Списки смежности**  
Удобны для разреженных графов (мало рёбер). Представляют собой массив, где каждая вершина хранит список всех смежных ей вершин (для ориентированного графа — вершины, в которые из неё можно попасть).

Пример для графа:
$[a]->a:2->b:5->d:8$
$[b]->c:7->d:9$
$[c]->d:4$
$[d]->c:3$

Если вершина a соединена с b, c и d, то список смежности выглядит как:
$[a] -> b, c, d$

##### 2.    **Списки рёбер**  
Списки рёбер хранят массив всех рёбер г рафа в виде пар вершин и их веса (если граф взвешенный). Подходит для графов с небольшим количеством рёбер

Пример:
Ребро между a и b с весом $5 -> [a, b, 5]$

$[a,a,2], [a,b,5], [a,d,8], [b,c,7], [b,d,9], [c,d,4], [d,c,3]$

##### 3.    **Матрица смежности**
Матрица размером $|V|\times|E|$, где элемент $(i, j)$ равен весу ребра между 
вершинами *i* и *j. Если ребра нет, значение равно 0 или бесконечности (∞). На пересечении двух вершин указывается вес ребра (или 1, если веса нет). 
Это представление удобно для проверки наличия рёбер между вершинами за $O(1)$ , но занимает много памяти.

Пример:
```
    a  b  c  d  
a   2  5  0  8  
b   0  0  7  9  
c   0  0  0  4  
d   0  0  3  0  
```


##### 4.    **Матрица инцидентности**
Матрица размером $|V|\times|E|$ , где строки соответствуют вершинам, а столбцы — рёбрам. Значение показывает, инцидентно ли ребро данной вершине, и в каком направлении (для ориентированных графов).

Пример:

```
       a  b  c  d  
(a,a)  2  0  0  0  
(a,b)  0  5  0  0  
(a,d)  0  0  0  8  
(b,c)  0  0  7  0  
(b,d)  0  0  0  9  
(c,d)  0  0  0  4  
(d,c)  0  0  3  0  

```

##### Поиск в ширину (BFS)

BFS — это алгоритм обхода графа, который проходит вершины по уровням, начиная с указанной стартовой вершины.

Пусть задан невзвешенный ориентированный граф $G=(V,E)$, в котором выделена исходная вершина $s$. 
Требуется найти длину кратчайшего пути (если таковой имеется) от одной заданной вершины до другой. Частным случаем указанного графа является невзвешенный неориентированный граф, т.е. граф, в котором для каждого ребра найдется обратное, соединяющее те же вершины в другом направлении.

Для алгоритма нам потребуются очередь и множество посещенных вершин $was$, которые изначально содержат одну вершину $s$. На каждом шагу алгоритм берет из начала очереди вершину $v$ и добавляет все не посещенные смежные с $v$ вершины в $was$ и в конец очереди. 
Если очередь пуста, то алгоритм завершает работу.

###### Анализ времени работы
Оценим время работы для входного графа $G=(V,E)$, где множество ребер E представлено списком смежности. 
В очередь добавляются только не посещенные вершины, поэтому каждая вершина посещается не более одного раза. Операции внесения в очередь и удаления из нее требуют $O(1)$ времени, так что общее время работы с очередью составляет $O(|V|)$ операций. Для каждой вершины $v$ рассматривается не более $deg (v)$ ребер, инцидентных ей. 
Так как $\sum_{v\in V} ^{} deg(v)=|E|$, то время, используемое на работу с ребрами, составляет $O(E)$. Поэтому общее время работы алгоритма поиска в ширину — $O(|V|+|E|$)$

1.    **Как работает**:
	- Начинаем с заданной вершины, добавляем её в очередь
	- Вытаскиваем вершину из очереди, посещаем её соседей (если они ее не посещены), и добавляем их в очередь
	- Повторяем пока очередь не опустеет

2.    **Применение**:
	- Найти все вершины, достижимые из начальной
	- Найти кратчайший путь в **невзвешенном** графе (количество ребер минимальное)

3.    **Пример на графе**:  
Если у нас граф:
```
0 — 1 — 4 
| 
2 — 3 
```

BFS от вершины 0:
	- Начинаем с 0, добавляем её в очередь
	- Посещаем её соседей: 1, 2 — добавляем в очередь
	- Берем из очереди 1, посещаем её соседа 4
	- Берем 2, посещаем её соседа 3

Итоговый порядок посещения: 0 → 1 → 2 → 4 → 3.

###### Кратчайший путь в невзвешенном графе (на основе BFS)
Когда рёбра графа равны по стоимости (например, каждая связь между вершинами имеет вес 111), BFS может быть использован для нахождения кратчайшего пути от начальной вершины до всех остальных.

1.    **Основа**:
	- BFS автоматически проходит вершины в порядке увеличения возрастания
	- Мы храним массив расстояний, где $distanse[v]$ — минимальное количество рёбер от начальной вершины до вершины $v$
2.    **Как это делается**:
	- Начитаем с начальной вершины, присваиваем её расстояние 0
	- Посещая соседние вершины, увеличиваем расстояние на 1
	- Повторяем пока не обработаем все вершины
3.    **Пример на графе**:
```
0 — 1 — 4 
| 
2 — 3 
```
Пусть начальная вершина 0.
o    На старте: d $distanse[0] = 0$ , остальные — бесконечность (∞).
o    Соседи 0: вершины 1 и 2,$distanse[1]=1$, $distanse[2]=1$
o    Соседи 1: вершина 4, $distanse[4]=2$
o    Соседи 2: вершина 3, $distanse[3]=2$

Итоговые расстояния:
$distanse[0] = 0$
$distanse[1] = 1$
$distanse[2] = 1$
$distanse[3] = 2$
$distanse[4] = 2$

Таким образом:
- BFS — это универсальный алгоритм для обхода графа и поиска кратчайшего пути в невзвешенных графах.
- Способы хранения графов (списки, матрицы) выбирают в зависимости от плотности графа и требований к скорости.

## 33. Описания способов хранения графов, а также реализация поиска в глубину и алгоритма топологической сортировки

[Материалы на сайте Пелевина](https://markoutte.me/students/graphs/)

> [!У графов есть несколько возможных способов хранения]
> 1. [[#Список смежности]]
> 2. [[#Список рёбер]]
> 3. [[#Матрица смежности]]
> 4. [[#Матрица инцидентности]]
#### Исходный граф для примера:
![[graph.svg]]
#### Список смежности 
представляет собой список, хранящий информацию из какой вершины в какую можно попать. А так же хранит вес ребра, пути к этой вершине. Схематично он выглядит вот так.

$$[a] -> a:2 -> b:5 -> d:8$$
$$[b] -> c:7 -> d:9$$
$$[c] -> d:4$$
$$[d] -> c:3$$
В коде же его можно реализовать как лист, храняший в себе другой лист.
Структура элемента главного листа выглядит так:
```python
class Data1:
    def __init__(self, vertex):
        self.vertex = vertex
        self.secondList = []
        self.next = None
```
То есть в каждом элементе списка мы храним вершину и список, в котором будет показано с какими ещё вершинами она будет связана.

Структура элемента из второго списка:
```python
class Data2:
    def __init__(self, value, vertex):
        self.value = value
        self.vertex = vertex
        self.next = None
```


#### Список рёбер
каждым своим элементом показывает наличие ребра (пути) между вершинами и вес этого ребра. Схематично он выглядит вот так:
$$[a,a,2]$$
$$[a,b,5]$$
$$[a,d,8]$$
$$[b,c,7]$$
$$[b,d,9]$$
$$[c,d,4]$$
$$[d,c,3]$$
То есть по сути это:
```python
std::list<std::vector> listOfEdges;
```


#### Матрица смежности
это матрица размеров $|V| \times |V|$ , где  — $|V|$ кол-во вершин в графе. В этой матрице на пересечении для каждой пары вершин стоит либо 0/1, обозначая наличие пути как таковое, либо вес ребра.
Выглядит матрица вот так:
$$
\begin{bmatrix}
  & a & b & c & d \\
a & 2 & 5 & 0 & 8 \\
b & 0 & 0 & 7 & 9 \\
c & 0 & 0 & 0 & 4 \\
d & 0 & 0 & 3 & 0 \\
\end{bmatrix}
$$
В коде её можно реализовать просто как массив масивов или список списков.

#### Матрица инцидентности
 имеет размер $|V| \times |E|$ где $|E|$ — кол-во рёбер. Пересечения говорят о том, что данное ребро инцидентно вершине (то есть один из её концов содержит вершину). В случае ориентированного графа в матрице смежности используются положительные числа для того, чтобы обозначить, что ребро выходит из этой вершины.
 Выглядит она вот так:
 $$
\begin{bmatrix}
 & a & b & c & d \\
(a,a) & 2 & 0 & 0 & 0 \\
(a,b) & 0 & 5 & 0 & 0 \\
(a,d) & 0 & 0 & 0 & 8 \\
(b,c) & 0 & 0 & 7 & 0 \\
(b,d) & 0 & 0 & 0 & 9 \\
(c,d) & 0 & 0 & 0 & 4 \\
(d,c) & 0 & 0 & 3 & 0 \\
\end{bmatrix}
$$
В коде её можно реализовать просто как массив масивов или список списков.
#### Поиск в глубину (DFS)

[Хабр](https://habr.com/ru/articles/504374/)

Сам алгоритм выглядит так:
1. Начинаем в какой-то точке
2. Из этой точки идём в "первого" её соседа. Так мы делаем до тех пор, пока мы не дойдём либо до искомой точки, либо то "конца пути" — точки у которой мы посетили всех соседей.
3. Если мы находимся в этаком листе (в конечной точке), а нужная точка ещё не найдена или граф не весь исследован, то мы возвращаемся назад до тех пор, пока не найдём точку у которой есть сосед, к которому мы ещё не ходили, а дальше goto п.2.

Реализация на C++:
```python
def DFS(node, graph):
    visited = [False] * len(graph)
    
    def dfs_util(v):
        visited[v] = True
        print(f"Visited node: {v}")
        
        for neighbor in graph[v]:
            if not visited[neighbor]:
                dfs_util(neighbor)
    
    dfs_util(node)


def main():
    # Пример графа (список смежности)
    graph = [
        [1, 2],    # Вершина 0 соединена с 1 и 2
        [0, 3, 4], # Вершина 1 соединена с 0, 3 и 4
        [0],       # Вершина 2 соединена с 0
        [1],       # Вершина 3 соединена с 1
        [1]        # Вершина 4 соединена с 1
    ]
    
    # Запуск DFS с вершины 0
    DFS(0, graph)


if __name__ == "__main__":
    main()

```


#### Топологическая сортировка (на основе DFS)

[Хабр](https://habr.com/ru/articles/100953/)

Сам алгоритм выглядит так:
1. Запускаем DFS
2. Когда мы дошли до "конечной вершины" — вершины у которой либо нет соседей, либо мы всех посетили, мы добавляем вершину в стек.
3. Когда такой модифицированный DFS закончил работу, мы достаём вершины из стека в обратном порядке. Всё.

```python
from collections import defaultdict

def DFS(node, graph, visited, result):
    """Рекурсивный обход в глубину с сохранением топологического порядка"""
    visited[node] = True
    print(f"Visited node: {node}")
    
    # Рекурсивно обходим всех соседей
    for neighbor in graph[node]:
        if not visited[neighbor]:
            DFS(neighbor, graph, visited, result)
    
    # После обработки всех соседей добавляем вершину в стек (результат)
    result.append(node)


def topologicalSort(graph):
    """Выполняет топологическую сортировку графа"""
    graphSize = len(graph)
    visited = [False] * graphSize  # Массив для отслеживания посещённых вершин
    result = []                    # Список для хранения топологического порядка
    
    # Запускаем DFS для всех непосещённых вершин
    for i in range(graphSize):
        if not visited[i]:
            DFS(i, graph, visited, result)
    
    # Разворачиваем результат (так как вершины добавлялись в конец)
    return result[::-1]


def main():
    # Пример графа (список смежности)
    graph = [
        [1, 2],    # Вершина 0 соединена с 1 и 2
        [3, 4],    # Вершина 1 соединена с 3 и 4
        [],        # Вершина 2 не имеет исходящих рёбер
        [],        # Вершина 3 не имеет исходящих рёбер
        []         # Вершина 4 не имеет исходящих рёбер
    ]
    
    # Выполняем топологическую сортировку
    order = topologicalSort(graph)
    
    # Вывод результата
    print("Topological order:", *order)


if __name__ == "__main__":
    main()
```

## 34. Описание и реализация алгоритма Прима для поиска минимального остовного дерева

Предназначен для поиска минимального остовного дерева в связном неориентированном графе. 
На каждом шаге к дереву добавляется легкое ребро, соединяющее дерево и отдельную вершину из оставшейся части графа. Данная стратегия является жадной, поскольку на каждом шаге к дереву добавляется ребро, которое вносит минимально возможный вклад в общий вес.
**Сложность** — $O(E\log V)$, где E - количество рёбер, V - количество вершин

Алгоритм:
Начинаем с производной начальной вершины и последовательно добавляем к текущему дереву минимальное по весу ребро. Добавляемое ребро должно соединять одну из вершин текущего дерева с одной из вершин еще не принадлежащих дереву.
Процесс повторяется, пока все вершины не будут включены в дерево.
собственно
- Инициализация
	- Для каждой вершины графа кроме корня задаем "$\infty$" , корень = 0
	- Вершины добавляются в очередь с приоритетами pq, приоритет определяется значением $key$
- Основная часть
	- Из очереди $pq$ извлекается вершина $u$ (с минимальным ключом) и добавляется в остовное дерево
	- Для каждой смежной вершины $v$, которая еще не включена в дерево:
		- Если вес ребра $(u, v)$ меньше текущего значения $key[v]$, то обновляется:
			- $key[v]$: вес ребра $(u,v)$
			- $\pi[v]$: родитель вершины v устанавливается равным u
- Завершение
	- Алгоритм завершается, когда очередь $pq$ становится пустой.
	- Остовное дерево формируется множеством пар $(v, \pi[v])$, где $\pi[v]$ — родитель вершины $v$


Реализация:
```python
import heapq
import sys

def MST_PRIM(graph, start):
    """
    Алгоритм Прима для нахождения минимального остовного дерева
    
    Args:
        graph: список смежности графа в формате [[(вершина, вес), ...], ...]
        start: начальная вершина
    """
    n = len(graph)  # количество вершин в графе
    key = [sys.maxsize] * n  # значения ключей вершин
    parent = [-1] * n  # массив для хранения родителя каждой вершины
    inMST = [False] * n  # включена ли вершина в остовное дерево
    key[start] = 0  # ключ для начальной вершины равен 0
    
    # Очередь с приоритетами, где храним пары (ключ, вершина)
    pq = []
    heapq.heappush(pq, (0, start))
    
    while pq:  # пока очередь не пуста
        current_key, u = heapq.heappop(pq)  # извлекаем вершину с минимальным ключом
        
        if inMST[u]:
            continue  # пропускаем уже обработанные вершины
        
        inMST[u] = True  # помечаем вершину как включенную в остовное дерево
        
        # Обрабатываем соседей вершины u
        for v, weight in graph[u]:
            if not inMST[v] and weight < key[v]:
                parent[v] = u  # обновляем родителя
                key[v] = weight  # обновляем ключ
                heapq.heappush(pq, (key[v], v))  # добавляем вершину в очередь
    
    # Вывод минимального остовного дерева
    print("Edge  : Weight")
    for i in range(n):
        if parent[i] != -1:
            print(f"{parent[i]} - {i} : {key[i]}")


# Пример использования
if __name__ == "__main__":
    # Пример графа в формате списка смежности
    graph = [
        [(1, 2), (2, 3)],      # вершина 0 соединена с 1 (вес 2) и 2 (вес 3)
        [(0, 2), (2, 1), (3, 1)], # вершина 1 соединена с 0 (2), 2 (1), 3 (1)
        [(0, 3), (1, 1), (3, 5)], # вершина 2 соединена с 0 (3), 1 (1), 3 (5)
        [(1, 1), (2, 5)]       # вершина 3 соединена с 1 (1) и 2 (5)
    ]
    
    MST_PRIM(graph, 0)
```


## 35. Описание и реализация алгоритма Крускала для поиска минимального остовного дерева

Алгоритм Крускала — это жадный алгоритм для нахождения минимального остовного дерева (МОД) в взвешенном графе. МОД — это подграф, соединяющий все вершины графа, в котором сумма весов рёбер минимальна, и в нём отсутствуют циклы.

**Описание алгоритма**
1.    **Инициализация**:
	- Сформировать список всех ребер графа, отсортированных по весу
	- Каждую вершину поместить в свою отдельную компоненту (множество)
2.    **Выбор рёбер**:
	- Итерироваться по отсортированному списку рёбер
	- Для каждого ребра проверить, принадлежат ли его концы одной компоненте (проверка на цикл)
	- Если рёбра соединяют разные компоненты, добавить ребро в остовное дерево и объединить компоненты
3.    **Завершение**:
	- Процесс продолжается, пока не будет добавлено $V-1$ (где $V$ — количество вершин графа)

**Характеристики алгоритма**
- **Сложность**:
	- Основное время уходит на сортировку рёбер: $O(E\log E)$, где E — количество рёбер
	- Операции объединения и проверки на принадлежность компоненте выполняются за амортизированное время $O(\alpha(V))$, где $\alpha$ — обратная функция Аккермана

- **Применение**: Используется для разреженных графов, где $E ≪ V^2$.
- **Жадность**: Алгоритм следует жадному подходу, выбирая на каждом шаге ребро с минимальным весом.

---

#### Реализация:
```python
class UnionFind:
    """Система непересекающихся множеств для проверки циклов в графе"""
    def __init__(self, n):
        self.parent = list(range(n))  # каждый элемент - свой представитель
        self.rank = [0] * n  # ранг для оптимизации объединения
    
    def find(self, x):
        """Находит представителя множества с применением сжатия пути"""
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])
        return self.parent[x]
    
    def unite(self, x, y):
        """Объединяет множества, содержащие x и y"""
        rootX = self.find(x)
        rootY = self.find(y)
        
        if rootX != rootY:
            # Объединение по рангу для оптимизации
            if self.rank[rootX] > self.rank[rootY]:
                self.parent[rootY] = rootX
            elif self.rank[rootX] < self.rank[rootY]:
                self.parent[rootX] = rootY
            else:
                self.parent[rootY] = rootX
                self.rank[rootX] += 1


# Предположим, что Edge определен как класс
class Edge:
    def __init__(self, u, v, weight):
        self.u = u
        self.v = v
        self.weight = weight
    
    # Метод для сравнения ребер по весу (для сортировки)
    def __lt__(self, other):
        return self.weight < other.weight


def kruskal(n, edges):
    """
    Алгоритм Крускала для нахождения минимального остовного дерева
    
    Args:
        n: количество вершин в графе
        edges: список ребер графа
        
    Returns:
        Кортеж (минимальное остовное дерево, общий вес)
    """
    # Сортируем ребра по весу по возрастанию
    edges.sort()
    
    uf = UnionFind(n)  # создаем систему непересекающихся множеств
    mst = []  # список ребер минимального остовного дерева
    mst_weight = 0  # общий вес минимального остовного дерева
    
    for edge in edges:
        # Проверяем, принадлежат ли вершины разным компонентам связности
        if uf.find(edge.u) != uf.find(edge.v):
            uf.unite(edge.u, edge.v)  # объединяем компоненты
            mst.append(edge)  # добавляем ребро в остовное дерево
            mst_weight += edge.weight
    
    return mst, mst_weight

```
